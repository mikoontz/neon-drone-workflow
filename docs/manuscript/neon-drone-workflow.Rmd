---
bibliography: ../neon-drone-workflow.bib
csl: ../ecosphere.csl
params:
  title: "Democratizing macroecology: integrating uncrewed aerial systems with the National Ecological Observatory Network"
  author: |
    Michael J. Koontz^1\*^,
    Victoria M. Scholl^1,2,3^,
    Anna I. Spiers^1,4^,
    Megan E. Cattau^5^,
    John Adler^2,6^,
    Joseph McGlinchy^1^,
    Tristan Goulden^6^,
    Brett A. Melbourne^4^,
    Jennifer K. Balch^1,2^
  affiliation: |
    ^1^Earth Lab, University of Colorado; Boulder, CO, USA  
    ^2^Department of Geography, University of Colorado; Boulder, CO, USA  
    ^3^Geosciences and Environmental Change Science Center, U.S. Geological Survey; Lakewood, CO, USA  
    ^4^Department of Ecology and Evolutionary Biology, University of Colorado; Boulder, CO, USA  
    ^5^Department of Human-Environment Systems, Boise State University; Boise, ID, USA  
    ^6^Battelle, National Ecological Observatory Network (NEON); Boulder, CO, USA  

  correspondence: |
    mikoontz@gmail.com
  keywords: |
    NEON, macroecology, UAS, drone    
  date_generated: !r format(Sys.Date(), "%B %d, %Y")
  
geometry: margin=1in
header-includes:
  - \usepackage[left]{lineno}
  - \linenumbers
  - \usepackage{setspace}
  - \doublespacing
  - \DeclareUnicodeCharacter{200E}{}
  - \usepackage{caption}
  - \captionsetup[figure]{labelformat=empty}
  - \captionsetup[table]{labelformat=empty}

output: pdf_document
---

```{r setup, include = FALSE, echo = FALSE}
knitr::opts_chunk$set(echo = FALSE, include = FALSE)
```

# `r params$title`

`r params$author`

`r params$affiliation`

*^\*^Correspondence:* ``r params$correspondence``

*Keywords:* `r params$keywords`

Date report generated: `r params$date_generated`

```{r libraries}
library(tidyverse)
library(pander)
library(captioner)
```

```{r captions}
fig_nums <- captioner(prefix = "Figure")
table_nums <- captioner(prefix = "Table")
eq_nums <- captioner(prefix = "Equation")
```

## Abstract 

Macroecology research seeks to understand ecological phenomena with causes and consequences manifesting at spatial scales spanning several orders of magnitude.
Interactions between individual organisms just a few meters away from each other can be relevant for explaining regional biogeography across hundreds of kilometers, and vice versa. 
Broad-extent, fine-grain (i.e., high resolution) information is needed to adequately capture these cross-scale interactions and emergences, but these data are costly to acquire and process. 
Uncrewed aerial systems (UAS, or drones carrying a sensor payload) and the National Ecological Observatory Network (NEON) make the broad-extent, fine-grain observational domain more accessible to researchers.
Integrating these tools can further democratize macroecological research by enabling researchers to flexibly and affordably fill spatiotemporal gaps in data collected by traditional means. 
However, using these tools for macroecology can be challenging, and mental models are lacking. 
Building mental models from scratch incurs costs of time, energy, and creativity which are likely better spent advancing science. 
This inspired a working group of drone-using academic ecologists, NEON professionals, imaging scientists, remote sensing specialists, and aeronautical engineers at the 2019 NEON Science Summit in Boulder, CO to create accessible recommendations and a sample workflow for ecologists new to drones and NEON. 
Specifically, we provide (1) a collection of best practices and practical considerations for collecting high-quality UAS data and (2) a sample workflow to process UAS data into meaningful ecological information and integrate it with NEON data collected on the ground with the Terrestrial Observation System and remotely from the Airborne Observation Platform. 
With this mental model, we advance the democratization of macroecology by making a key observational domain-- broad-extent, fine-grain data-- more accessible via NEON/UAS integration.

## Introduction

Macroecology is the study of spatially extensive systems whose biological, geophysical, and social components interact dynamically both within and across spatiotemporal scales [@heffernan2014].
Macroecology, in its explicit consideration of scale, extends from a rich history of basic ecological research seeking to explain patterns in nature [@turner1989; @levin1992]. 
At the same time, macroecology is highly relevant to applied ecology, as the broader spatial extents studied reflect the scale at which many societally relevant challenges, and perhaps their solutions, arise [@heffernan2014; @larue2021]. 
The causes and consequences of phenomena under investigation in macroecology can span many spatial scales, which motivates a characteristic feature of the data to be brought to bear: they must often be simultaneously fine in grain (i.e., resolution) and broad in extent [@beck2012].

Ecologists typically face a data collection tradeoff between grain and extent that constrains the observational domain of their research [@ernest2018; @estes2018]. 
Indeed, the spatial and temporal observational domains of most ecology research is narrow [@estes2018]
The grain/extent tradeoff can sometimes be overcome, but at a high cost.
For example, the Global Airborne Observatory collects high spatial and spectral resolution data at broad extents [@asner2007; @asner2012], but the price of data acquisition and processing tallies in the millions of US dollars, even though the per-area cost is low [@asner2013].
As another example, the US Forest Service Forest Inventory and Analysis program maintains a regular network of over 350,000 fine-grain field plots regularly spaced over the entire forested area of the United States (over 9.1 million km2; approximately 1 plot every 2400 hectares) at an annual cost of tens of millions of dollars [@gillespie1999; @alvarez2020].
Most science studies have relatively modest budgets and are conducted by just a few individuals [@heidorn2008].
The modal NSF Division of Environmental Biology award was about $200,000 USD between 2005 and 2010 [@hampton2013].
While the fine-grain, broad-extent observational domain is invaluable for macroecology, it can be inaccessible to ecologists with resource or funding limitations.

Increasing accessibility of relevant scales of observation reduces barriers to research participation-- it democratizes macroecology [@guston2004].
Removing these barriers improves science because the rate, direction, and quality of science are, in part, shaped by the available research inputs [@nagaraj2020].
For instance, access to the archive of Landsat earth observation imagery was improved in 1995, which dramatically increased the quantity, quality, and diversity of Landsat-enabled science [@nagaraj2020].
The same archive became freely available in 2008 with concomitant benefits to projects that rely on Landsat observations [e.g., @picotte2020].
The changing accessibility of Landsat data is noteworthy for macroecology, as the archive provides global-extent, relatively fine-grain (30 meter) imagery since 1984.
Particularly when paired with the power of a planetary-scale geographic information system [such as Google Earth Engine\; @gorelick2017], Landsat imagery has led to breakthrough science that is “globally consistent and locally relevant” such as the first global map of forest cover changes over a decade-long period at a relatively fine scale [@hansen2013]. 

Accessible research stimulates revolutionary science.
In this way, the National Ecological Observatory Network (NEON) is revolutionary [@nsf2013; @balch2020].
NEON is a continental-scale observation facility in the United States comprising 81 sites within 20 ecoclimatically distinct domains and an operational lifespan on the order of decades [@schimel2013; @keller2008].
NEON is designed to collect rigorous, consistent, long-term, and open access data to better understand how U.S. ecosystems are changing, using a combination of field measurements by trained personnel, ground-based automated sensors, and the Airborne Observation Platform (AOP) which uses plane-based instruments to collect both active and passive remotely-sensed data [@kampe2010; @nsf2013].
NEON observations span spatial scales, from measurements of individual organisms within small field plots to 10 cm resolution RGB imagery and 1 m hyperspectral and lidar imagery across hundreds of square kilometers, with measurements replicated across  sites that span the  continental extent of the Observatory [@keller2008].
A stated goal of NEON is to democratize access to ecological research, particularly at broad extents [@nsf2013]-- its promise is continental-scale ecology for everyone.
NEON pairs publicly available data with a strong outreach and education effort to help realize this promise.
In this way, NEON broadens access to macroecology by reducing barriers to entry, particularly cost, fieldwork requirements, and technical expertise [@nagyInpress].
An “instrument” like NEON collecting standardized data at such scales leads to inevitable tradeoffs-- in the specific times, locations, and type of data that are sampled.
While the NEON data are on their own sufficient for advancing ecology, part of what makes NEON revolutionary is its foresight in facilitating connections to other ecological data.
In this way, the fundamental limitations of NEON can be overcome with bridges to more targeted ecological studies.

Uncrewed aerial systems (UAS) can also revolutionize ecology [@anderson2013b].
UAS are increasingly being used to collect high spatial resolution information over relatively large spatial extents for ecological science applications [@wyngaard2019].
A UAS comprises a vehicle and a payload.
The vehicle is also known as a “drone” or a “UAV” standing for  “uncrewed aerial vehicle”, “unhumanned aerial vehicle”, “unoccupied aerial vehicle”, or, more primitively, for “unmanned aerial vehicle”-- see @joyce2021.
The payload is the instrumentation carried by the vehicle beyond what is critical for flight operations, and gives the UAS its scientific value.
Importantly, it isn’t the drone itself that enables ecological studies at heretofore inaccessible scales, but rather the drone’s ability to position a data collecting payload (i.e., a sensor) in a repeatable, efficient, hard-to-reach manner.
For example, one use case for UAS is structure from motion (SfM) photogrammetry which generates a 3-dimensional model of an area of interest using 2-dimensional images from multiple overlapping viewing angles [@westoby2012].
The minimum requirement for SfM photogrammetry is 2-dimensional imagery, which can be captured from the ground using a hand-held sensor (e.g., a digital camera) to great effect for some applications [@piermattei2019].
A drone-based camera can capture imagery from higher up in, or above, the canopy, which allows for measurement of higher vegetation strata [@kuzelka2018], including total height for above-canopy applications.
Drone-based SfM photogrammetry also increases the extent that can be covered with surveys [@jackson2020], since aerial transects are unimpeded by varied terrain and vegetation encountered on ground transects.
Unimpeded aerial transects are also more reliably repeated than ground surveys that require navigating through vegetation and are likely to be less impactful to that vegetation.
UAS provide an avenue to flexibly and affordably fill spatiotemporal gaps in data collected by traditional means-- they can be deployed more frequently and capture finer grain data than airplane- and satellite-based platforms, and can cover greater extents than ground surveys.
While UAS-derived data have the potential to revolutionize spatial ecology, a lack of standards and practical workflows for collecting and processing UAS data challenges the realization of this potential [@assmann2019; @wyngaard2019].

UAS and NEON complement each other.
Each can be a key tool for macroecology research, but their integration offers an opportunity to alleviate some of their fundamental constraints in a similar way as integration of NEON with other earth observing networks.
NEON data derive from "state of the science" instrumentation with thorough documentation, and are standardized at a continental scale.
NEON data collection is pre-planned, which makes the resulting data somewhat predictable, but also rigid in space, time, and type.
On the other hand, UAS operations are nimble and customizable, but the resulting data are relatively under-validated with data standards that are ad-hoc, idiosyncratic, and lacking in a consistency which makes interoperability of those data across projects a challenge [@wyngaard2019].
Realization of the benefits of UAS-NEON integration by ecologists is challenged by the relative novelty of these tools and associated workflows [@nagyInpress].
Not knowing where to start with not one, but two new tools is a daunting proposition and gaining enough proficiency to use them in a research setting comes at the expense of doing research itself  [@olah2017].
Reducing these barriers to proficiency therefore has tremendous research value.

Mental models help novices become experienced practitioners by providing a contextual framework for new knowledge [@knapp2010].
We assembled a working group of participants at the 2019 NEON Science Summit in Boulder, Colorado with a goal to create accessible recommendations and a sample workflow for ecologists new to UAS to guide self-teaching and democratize the use of UAS to link with NEON as an ecology tool.
Here, we outline some core principles relating to integrating drone-derived data with the NEON observatory.
We motivate the considerations of each principle with a case study describing an example data collection mission and processing workflow that includes data and code.
We emphasize that project-specific science requirements, rather than rigid prescriptions, should guide the decision-making regarding these core considerations for drone-enabled ecology and integration with NEON: vehicle, environment, regulations, mission planning, payload, calibration, georeferencing, data management, and data processing (including integration with NEON field and AOP data).

In this work, we aim to effectively lower the barrier to entry for using UAS and NEON to do ecology.
Specifically, we focus on optical data collected by each tool over terrestrial sites and provide (1) a collection of practical considerations for collecting high-quality UAS data to integrate with NEON field and AOP data and (2) a real-world workflow depicting the processing of UAS data into meaningful ecological information at the NEON Niwot Ridge (NIWO) field site.

## Core considerations for NEON/drone integration

### Science requirements

Both NEON- and UAS-enabled ecology have faced criticism for having “backwards” approaches to science.
Critics suggest that clear science questions take a back seat to high-quality but rote data collection in the case of NEON [@lindenmayer2018] or perpetual methodological refinement in the case of UAS [@gillan2021].
In light of this, we echo @assmann2019 in emphasizing that the main consideration to keep in mind while when integrating UAS with NEON is: what are the science requirements and what data collection efforts are "good enough" to meet those requirements?
A clear science question helps guide the data collection needs, which can minimize the amount of researcher time and energy spent on developing tools that ultimately prove to be superfluous [@joseph2021].

### Vehicle

The vehicle in a UAS is the flying machine that holds the payload.
One key distinction between vehicle types is whether rotor systems or fixed wings are used for lift (the upward force that keeps the vehicle in the air).
Rotocopter vehicles (also known as “multicopters”, “multirotors”, “quadcopters”, “hexacopters”, or “octocopters” depending on the number of rotor systems) consist of a body and 4-8 rotary systems that provide both lift and thrust (horizontal motion).
These types of vehicles are characterized as “vertical takeoff and landing” (VTOL).
Fixed wing aircraft use wings for lift, and use rotor systems only for thrust.
Hybrid vehicles use rotor systems for lift during ascent and descent, but fixed wings for lift during flight and are sometimes referred to as VTOL fixed wing systems to highlight this combination of features.
The structure and size of the vehicle determine its functionality in the field and thus a project’s objectives can often help constrain the choices available.
Rotocopter platforms are more maneuverable, often less expensive, easier to fly, more transportable, and have a higher payload capacity relative to fixed wing aircraft.
For these reasons, rotocopters are often preferred by ecologists.
On the other hand, fixed wing aircraft have longer flight times with better battery usage and thus can cover larger areas more efficiently than rotocopters.
For example, covering the full extent of a given AOP footprint (147.6 +/- 107.2  km^2^ for core and relocatable sites) may be most efficiently conducted with a fixed-wing or hybrid vehicle.
They are also more stable in adverse conditions (e.g., high winds) and have a safer recovery from motor power loss.
VTOL fixed wing systems can combine the efficiency of a fixed wing with the small takeoff/landing footprint of a rotocopter.

A flat surface clear of obstructions (e.g., on dirt rather than grass, away from canopy) is ideal for UAS take-offs and landings.
VTOL systems require a smaller take-off and landing footprint, which may be satisfied with only a small canopy gap, compared to vehicles that use fixed wings for lift, which require a “runway” for  fixed wing vehicles.
Locating such a site may be challenging at some NEON sites (e.g., NIWO, with dense canopy cover) and easy at others (e.g., SJER, with an open woodland ecotype).
Bringing something that can provide a clean surface will prevent dirt from obstructing or scratching the sensor lens; for example, a user may bring a car trunk’s mat or plywood as a reliable take-off and landing surface.
With any platform, the vehicle endurance and the mission goals are likely to require purchase of several batteries to keep the vehicle flying for the necessary duration of a field day.
A user may consider an energy source to charge batteries in the field, like a solar charger or generator, or may just invest in several batteries to have fully charged each day.

<!-- ```{r} -->
<!-- table_nums(name = "table-vehicle", "Summary of vehicle considerations for UAS-enabled ecology.") -->
<!-- ``` -->

<!-- ```{r vehicle_table_print, echo = FALSE, include = TRUE, results = 'asis'} -->
<!-- vehicle_table_print <- -->
<!--   tibble::tibble(Consideration = c("", "", "", "Vehicle", "", "", ""),  -->
<!--                  Options = c("Rotocopter", -->
<!--                              "", -->
<!--                              "", -->

<!--                              "Fixed wing",  -->
<!--                              "", -->

<!--                              "VTOL fixed wing", -->
<!--                              "(Hybrid)"), -->
<!--                  Advantages = c("Ease of take-off and landing", -->
<!--                                 "Hover capability, maneuverability", -->
<!--                                 "Affordable",  -->

<!--                                 "Longer flight time (2+ hours)", -->
<!--                                 "Cover large spatial extent",  -->

<!--                                 "Simpler takeoff/landing", -->
<!--                                 "Longer flight time"), -->
<!--                  Disadvantages = c("Shorter flight time (~20 minutes)",  -->
<!--                                    "", -->
<!--                                    "", -->

<!--                                    "Minimum flight speed to keep it aloft (affecting overlap, image quality)", -->
<!--                                    "Complex takeoff/landing",  -->

<!--                                    "Newer technology", -->
<!--                                    "Expensive"), -->
<!--                  `Reference(s)` = c("", "", "", "@anderson2013b; @goodbody2017; @padua2017", "", "", "")) -->

<!-- pandoc.table(vehicle_table_print,  -->
<!--              split.tables = Inf, -->
<!--              caption = table_nums(name = "table-vehicle"), -->
<!--              keep.line.breaks = TRUE) -->
<!-- ``` -->

```{r vehicle_payload_table_print, echo = FALSE, include = TRUE, results = 'asis'}
vehicle_payload_table_print <-
  tibble::tibble(Consideration = c("", "", "", "Vehicle", "", "", "",
                                   "", "", "", "Payload", "", "", ""),
                 Options = c("Rotocopter",
                             "",
                             "",
                             "Fixed wing",
                             "",
                             "VTOL fixed wing",
                             "(Hybrid)",
                             "RGB camera",
                             "",
                             "",
                             "Multispectral sensor",
                             "",
                             "Hyperspectral/",
                             "Imaging spectrometer"),
                 Advantages = c("Ease of take-off and landing",
                                "Hover capability",
                                "maneuverability",
                                "Affordable",

                                "Longer flight time (2+ hours)",
                                "Cover large spatial extent",

                                "Simpler takeoff/landing",
                                "Longer flight time",
                                "Small size, Affordable",
                                "Fine spatial resolution",

                                "Small size",
                                "More precise spectral information",

                                "High spectral resolution",
                                "High spectral extent"),
                 Disadvantages = c("Shorter flight time (~20 minutes)",
                                   "",
                                   "",

                                   "Minimum flight speed to keep it aloft (affecting overlap, image quality)",
                                   "Complex takeoff/landing",

                                   "Newer technology",
                                   "Expensive",
                                   "Limited spectral extent to visible wavelengths",
                                   "",
                                   "",

                                   "Limited spectral sampling typically in visible and infrared wavelengths",
                                   "Complex data acquisition and post-processing",

                                   "Heavy",
                                   "Expensive"),
                 `Reference(s)` = c("", "", "", "@anderson2013b; @goodbody2017; @padua2017", "", "", "",
                                    "@padua2017", "", "", "@padua2017", "", "", "@adao2017"))

pander::pandoc.table(vehicle_payload_table_print,
             split.tables = Inf,
             caption = table_nums(name = "table-vehicle-payload"),
             keep.line.breaks = TRUE)
```
### Payload

The payload is the equipment carried by the UAS that collects data and combines with the vehicle to constitute the system (the "S" in "UAS").
In fact, despite the typical focus on the drone vehicle, the payload is at least as an important component, since the main purpose of the vehicle is merely to position the payload where it needs to be in order to capture appropriate data.
For ecologists interested in optical data, the payload may be a simple camera or a more general remote sensing sensor sensitive to reflectance of electromagnetic light.
The scientific questions will dictate the data requirements, which will in turn drive the payload decision.
Typically, the selection of a sensor represents a trade-off between spatial resolution (the size of pixels in the imagery at a set altitude), spectral resolution (the number of distinct portions of the electromagnetic spectrum that the sensor can detect), spectral extent (how much of the electromagenetic spectrum the sensor can detect), and cost.
For example, while hyperspectral data provide high spectral resolution and extent that may allow you to identify the prevalence of specific chemical compounds in vegetation [e.g., foliar nitrogen; @knyazikhin2013], a multispectral instrument with fewer spectral channels [@koontz2021] or even an RGB camera [@scholl2020] may be more than sufficient for classifying vegetation to species.
Similarly, sensors with high spatial resolution can capture fine detail in their imagery but may reduce the ability to measure a variable of interest, such as individual trees, as post-processing steps can be negatively affected by the movement of those fine details in the wind [@young2021].
Hyperspectral instruments and high resolution cameras are relatively expensive in terms of purchase cost, post-processing time, and data storage requirements, but simple red-green-blue (RGB) and multispectral cameras can be affordably bought off-the-shelf so it is worth considering whether they would suffice for your particular scientific question.

It is also important to consider how the payload will be integrated with the vehicle, which generally requires considering the combination of the vehicle and payload simultaneously.
In some cases, the payload can operate entirely independently from the vehicle, and integration only requires a means of physically attaching the components together.
In other cases, the payload relies both on power and electronic signaling from the vehicle in order to capture data, and integration may require more specialized electrical and mechanical engineering expertise.
It's generally advisable to use a pre-built integration kit or an already-integrated sensor/vehicle system if the payload meets the science requirements (or nearly so).

### Environment

The environment of the UAS mission can affect both the equipment performance and the data collection such that the intended operation conditions must be considered during vehicle/payload selection and flight planning.
Foremost, the vehicle and the payload must be capable of functioning in the desired environment.
UAS flights at high elevations or in cold weather will drain battery faster than at sea level, and some popular vehicles won't allow take-off if the temperature is too cold (or hot).
While some vehicles are designed to withstand light precipitation and dust, many would be damaged under such flight conditions.
Heavy winds can push the UAS off-course or require the UAS to work harder to maintain its course, which drains battery faster and reduces endurance.
Variable terrain within the survey area may also affect vehicle endurance, as more energy is required to ascend and descend while also traversing along flight transects in the horizontal plane.
Managing the temperature of the mission critical electronics is just as important as that of the vehicle’s batteries during UAS operations.
The vehicle remote controller and any other peripherals such as a tablet computer are susceptible to extremely warm or cold temperatures drain battery life, and cold temperature can cause the vehicle and/or sensor to malfunction.
The mean annual temperature for NEON AOP sites ranges from -12°C at the Utqiaġvik site in Alaska to 25°C at Lajas Experimental Station in Puerto Rico (NEON Field Site Metadata; https://www.neonscience.org/sites/default/files/NEON_Field_Site_Metadata_20210226_0.csv; accessed 2021-03-16).
Expectations of unfavorable environmental conditions may be enough to dictate what equipment should comprise the UAS.
For example, high-wind conditions at NIWO may warrant a fixed wing platform; however, the dense forest would make take off and landing much easier with a rotocopter.
In some cases, steps can be taken to mitigate the unfavorable environmental conditions, such as keeping equipment out of direct sunlight to prevent overheating (to the point of adding sun umbrellas or shade tarps to the required equipment list) and storing batteries in a cooler when not in use in order to insulate them against temperature extremes.

Environmental conditions may also impact data collection on automated flights, particularly for optical data.
Ideal conditions for optical data collection are evenly lit with either complete cloud cover or clear skies.
If flying takes place under clear sky conditions, then the sun should be high in the sky so it doesn’t cast long shadows-- ideally within a couple of hours of solar noon (i.e., 10am and 2pm for standard time, and 11am to 3pm for regions that observe daylight savings time) [@assmann2019].
Note that some SfM software guidelines specifically suggest *not* flying near solar noon, as this can create particularly bright areas within each image that challenges the SfM algorithms (MapsMadeEasy; https://www.mapsmadeeasy.com/data_collection; accessed 2021-11-19).

Prior to flights, it's important to ensure that weather will be favorable for data collection.
A handheld instrument for measuring temperature, relative humidity, and wind speed may also aid in the reporting of flight conditions, though note that the wind speed at flight altitude may be different than what is measured on the ground.
In many cases, taking a picture of the sky and a screenshot of the weather forecast from a reputable source (e.g., NOAA) is a convenient and sufficient way to ensure later reporting on flight conditions.
In fact, the NEON AOP does exactly this for their daily flight reports.

### Flight planning

One of the key benefits of UAS operations is the ability to program missions to be automatically followed by the vehicle’s onboard flight software.
For optical data collection such as that required for structure from motion photogrammetry, the mission typically involves aerial transects with images captured at regular time or distance intervals so that objects in a scene are imaged from many viewing angles (often in excess of 100; (`r fig_nums(name = "fig-crosshatch", display = "cite")`)).
Successful flight planning requires consideration of the flight pattern, sensor angle, flight planning software, and operation routine.

Flight patterns are typically described in terms of their front overlap (a function of flight speed, flight altitude, frequency of image capture, and vertical field of view of the sensor) and side overlap (a function of flight altitude, horizontal field of view of the sensor, and distance between transects).
Higher overlap (in excess of 80% for both front and side overlap [@dandois2015] and even as high as 95% front overlap [@torres-sanchez2018; @frey2018] is required for successful photogrammetric reconstructions of more complex vegetation such as denser forests using commonly available photo processing software.
Lower overlap may be sufficient for 2-D mapping quality, though the processed product may not penetrate deeply into canopy gaps [@dandois2015] and image artifacts such as “leaning” objects which were only imaged from an oblique angle are more prevalent.
Additional overlap can be achieved by augmenting parallel transects with a second set of parallel transects rotated 90 degrees to the first (a cross hatch pattern; `r fig_nums(name = "fig-crosshatch", display = "cite")`).
Additional viewing angles can be achieved by tilting the sensor off nadir in order to capture oblique imagery which can aid in scene reconstruction [@james2014; @cunliffe2016].

Flight planning is typically achieved using specialized software, sometimes run on a separate device like a tablet computer.
Most flight software allows for setting the altitude as well as the desired forward and side overlap for a given aircraft and sensor.
Two other important software features that may routinely be relevant for ecology are terrain following and internet-free operations.
Terrain following enables the vehicle to ascend and descend to match topographic changes within the area of interest, such that approximately the same altitude above ground level is maintained throughout all aerial transects.
This serves two key functions: it ensures the safety of the vehicle and it maintains approximately the same ground sampling distance for imagery which aids in processing.
Some missions are most easily created once in the field in order to incorporate better information on the area of interest, takeoff/landing locations, and visibility throughout flight.
An ability for the software to function offline and to cache background map imagery can be critical for real-world UAS use.
Finally, flight software can be resource intensive enough to require higher computing power of the computer or tablet being used.
Crashing the flight software mid-flight due to compute resource overload is undesirable, so it is likely worth using a device with faster processors and/or more RAM.

A final consideration for successful flight planning is the routine used to execute missions.
Consistent repetition of routine steps prior to, during, and after a flight ensures that all components of the UAS work as intended in concert with each other, and checklists facilitate this consistency [@degani1993].
We highly recommend developing and using some kind of checklist for UAS operations such as the one we used for this manuscript (Supplementary Information)-- there is good reason they are part of standard operations for a range of aviators from pilots of small private aircraft to NEON AOP to NASA astronauts! Some applications (such as Kittyhawk; https://kittyhawk.io/) allow for automatic logging of checklist run throughs, which further reduces barriers to their use.
For our mission, we used Map Pilot for DJI by DronesMadeEasy, which enables full control of front/side overlap and camera angle, as well as allows for cross-hatch flight patterns, terrain following, and caching of data for use in the field when an internet connection isn’t available.

```{r}
fig_nums(name = "fig-crosshatch", "Black points depict the UAS position for each photo captured during the flight.
Red points in the “X” formation at the center are the high-precision geolocations of the NEON vegetation plot monuments.
The background color represents the approximate number of photos captured over each point in the surveyed area based on idealized image footprints projected on the ground surrounding the geolocation of each photo point (i.e., the black points).
Each point needs to be imaged a large number of times (likely more than 100 for denser vegetation), which means that some areas with image capture at the edges of the flown area won’t have coverage suitable for structure from motion data processing. 
Your flight area should therefore be larger than your area of interest to ensure sufficient data coverage.")
```

![`r fig_nums(name = "fig-crosshatch")`](../../figs/fig01_photo-points-overlap-and-gcps.png)

### Regulations (airspace and pilot)

In the United States, use of UAS is constrained by regulations on pilot credentials, airspace in which operations take place, and vehicle type.
This is an oft-cited hurdle to adoption of UAS for research use [@vincent2015].
There are currently three main sets of regulations for UAS operations within the United States: Part 107 for individuals, a Certificate of Authorization for organizations, and hobbyists who are flying strictly for recreational purposes.
Because the rules within each of these categories are actively changing, and drone pilots are responsible for staying aware of these updates.
Special waivers are needed to fly in "controlled" airspace (i.e., class B/C/D/E airspace, typically near airports), to fly a UAS above 55 lbs, and to fly a UAS beyond line-of-sight.
It is important to always call the appropriate land manager before flying on public land to obtain appropriate site access if necessary, to check for temporary closures (e.g., bird nesting), and to be a good neighbor.
Because NEON does not own the land on which they operate, flying NEON sites will require contacting and obtaining permission from the site host; contact information is available on the NEON webpage for each site, and NEON staff may also help facilitate those connections.
Additional, non-NEON research is allowed at some but not all sites.
If permission is obtained, it is important not to disturb any existing research being conducted at those sites, to maintain a 20m buffer around any NEON distributed plot, and to completely avoid the area of the tower airshed (which is also delineated on the NEON webpage for each site; e.g., https://www.neonscience.org/data-samples/data/spatial-data-maps).
Clear communication with concerned parties of UAS flights for research, even if you have every legal right to fly at a particular location, is important for building community credibility and longevity for UAS as a tool for ecologists.
The pilot should have an equipment list and an operations checklist to make sure safety procedures are followed.
Emergency operating procedures should be understood should a mishap occur, including: crash, battery fire, flyaway, airspace violation, or propellor injury.

```{r}
# table_nums(name = "table-regulations", "Summary of rules/regulations considerations for UAS-enabled ecology in the United States.")
```

```{r regulations_table_print, echo = FALSE, include = TRUE, results = 'asis'}
# regulations_table_print <-
#   tibble::tibble(Consideration = c("", "", "", "Vehicle", "", "", ""), 
#                  Options = c("Rotocopter",
#                              "",
#                              "",
#                              
#                              "Fixed wing", 
#                              "",
#                              
#                              "VTOL fixed wing",
#                              "(Hybrid)"),
#                  Advantages = c("Ease of take-off and landing",
#                                 "Hover capability, maneuverability",
#                                 "Affordable", 
#                                 
#                                 "Longer flight time (2+ hours)",
#                                 "Cover large spatial extent", 
#                                 
#                                 "Simpler takeoff/landing",
#                                 "Longer flight time"),
#                  Disadvantages = c("Shorter flight time (~20 minutes)", 
#                                    "",
#                                    "",
#                                    
#                                    "Minimum flight speed to keep it aloft (affecting overlap, image quality)",
#                                    "Complex takeoff/landing", 
#                                    
#                                    "Newer technology",
#                                    "Expensive"),
#                  `Reference(s)` = c("", "", "", "@anderson2013b; @goodbody2017; @padua2017", "", "", ""))
#   
# pandoc.table(regulations_table_print, 
#              split.tables = Inf,
#              caption = table_nums(name = "table-regulations"),
#              keep.line.breaks = TRUE)
```


```{r}
fig_nums(name = "fig-rededge-rsr-neon-aop", "a) Relative spectral response of the Micasense RedEdge 3 camera in 5 distinct spectral bands based on the quantum efficiency of the image sensor per wavelength and the bandpass filter transmission per wavelength. b) Relative spectral responses for two channels of the Micasense RedEdge 3 camera plotted with the relative spectral responses for twenty channels of the NEON AOP imaging spectrometer.
Several channels of the NEON AOP instrument comprise each of the Micasense RedEdge 3 channels, so the reflectance data from the NEON AOP are resampled (weighted, in effect) such that they can be used as though the NEON instrument exhibited the same spectral sensitivity as the Micasense RedEdge 3 instrument.")
```

![`r fig_nums(name = "fig-rededge-rsr-neon-aop")`](../../figs/fig02_relative-spectral-response-rededge3-and-neon-aop.png)

### Radiometric Calibration and Corrections

NEON implements a complex algorithm to convert its imaging spectrometer data to units of reflectance [@karpowicz2015].
This compensates for the scattering and absorption of light as it travels through the atmosphere (e.g., haze, water vapor) on its optical path to the AOP.
As individual researchers collecting UAS imagery, we are responsible for converting our otherwise arbitrary image pixel values into meaningful, standardized units like reflectance.
Applying image preprocessing steps (e.g., correcting for camera artifacts such as vignetting and dark noise) and subsequent radiometric calibration allows our UAS data to be comparable with high-quality scientific data products derived from the NEON AOP.
The Empirical Line Method (ELM) has proved to be a simple and accurate UAS radiometric calibration option [@wang2015].
ELM requires the placement of at least two materials such as calibrated reflectance panels with known reflectance in the scene, which are used to map image pixel values to reflectance for each spectral band.
Many sensor vendors and photogrammetry software options only provide or enable a single panel for radiometric calibration.

To give your UAS image data the best possible chances of being calibrated using any method in the future, we recommend using 3 panels with varying gray levels.
Ideally, your panels should be large enough to be imaged during flight and contain an area of 10 x 10 pixels [@wang2015].
Panels should be matte (as opposed to shiny or glossy) with a smooth, horizontal surface [@smith1999].
Your panel colors should be shades of black (near 0% reflectance) and gray, ideally covering the range of reflectance for your subject of interest.
White (near 100% reflectance) panels are not recommended since they can saturate and cause other issues [@cao2019].
 For plant surveys, we recommend a medium grey, dark grey, and black target since vegetation tends to be about 50% average reflectance or medium gray.
You can purchase calibrated reflectance panels from vendors, or make your own.
Many studies have identified promising materials for homemade panels: plywood covered with matte paint [@rosas2020], gray linoleum, and black fine-weave cotton fabric [@cao2019].
However, care must be taken with homemade panels because even though they may appear a particular shade to the human eye (visible spectrum), they may not be a similar reflectance across all wavelengths observed by a multi-spectral or hyperspectral sensor.
 
Regardless of panel cost, color, or material, it is critical to clean, re-measure, re-calibrate, and/or replace them over time to ensure the most accurate reflectance calibration possible for your UAS imagery.
This is especially important when your field work involves exposing panels to harsh environmental conditions with dirt, dust, sand, sun, and any other types of physical damage or degradation.
To illustrate this point, we re-measured a calibrated reflectance panel after three years of field work using a handheld Analytical Spectral Devices (ASD, ASD Inc., a Malvern PANalytical Company, Longmont, Colorado) FieldSpec 4 spectrometer (USGS data release DOI available upon publication).
We present the vendor-provided panel reflectance from the time of purchase in 2017 with the averaged ASD handheld reflectance collected three years later in 2020 (`r fig_nums(name = "fig-calibration-panel", display = "cite")`).
The reflectance of the panel has decreased by as much as 10% due to the presence of dirt and dust.
This decrease in reflectance is especially noticeable in the shorter wavelengths.
If we use the vendor-provided panel reflectance data from 2017 for current and future flights, we will introduce significant error into our reflectance calibration.
According to the manufacturer, this make and model of calibration panel is not able to be cleaned (though newer panels from this manufacturer can be cleaned; see https://support.micasense.com/hc/en-us/articles/360005163934-Calibrated-Reflectance-Panel-Care-Instructions) as this will force debris further into the pores of the panel material, so it is important to use the updated reflectance measurement for this panel during radiometric calibration.

NEON in essence does this scene-based radiometric calibration too.
A series of vicarious calibration flights are conducted with the NEON AOP before and after every field season [@leisso2014].
They fly over two large tarps with 48% (medium gray) and 3% (black) reflectance, collect ground truth reflectance measurements of these tarps with an ASD, and use these data to verify the radiometric calibration of the NEON AOP Imaging Spectrometer (https://www.neonscience.org/data-collection/imaging-spectrometer).
The reflectance of these tarps is meant to represent the upper and lower bounds of reflectance typically seen in nature.

We recognize that each researcher has vastly different constraints for their budget, environmental conditions in the field, and equipment availability, so “good enough” may be more realistically attainable than the “ideal” radiometric calibration practices described above.
Having one calibration panel is better than none; choose gray rather than white or black to avoid crushing or clipping in under/over exposed images.
Commercial SfM software like Agisoft Metashape and Pix4D accommodate at least one panel, so you can correct the imagery with a single panel although this may limit the data calibration possibilities using other methods.
If in-flight panel photos are not possible or if you have a small panel, be sure to take photos before/after flight.
A downwelling light sensor (DLS), also known as a sunshine sensor, records data about the current illumination levels during flight and is standard equipment on low cost multispectral sensors designed for agriculture.
The information collected by them is often incorporated into the SfM processing.
DLS data can help to correct each image based on changing light conditions during flight.

```{r}
fig_nums(name = "fig-calibration-panel", "Comparison of reflectance spectra (A) for a calibrated reflectance panel (B).
The panel vendor provides a reflectance spectrum describing the panel’s reflectance at the time of purchase (black reflectance profile, “Micasense, 2017”).
After three years of using the panel repeatedly for field work, we re-measured the panel using a handheld spectrometer (blue reflectance profile, “ASD, 2020”) to evaluate the change in spectral reflectance for the panel.
The spectra in (A) are reflectance values as a function of wavelength across the visible and near infrared.
The MicaSense RedEdge 3 spectral band ranges (blue, green, red, red edge, near infrared) are illustrated here as vertical bars of color.
The panel reflectance decreased between measurements collected in 2017 and 2020, with this decrease being more pronounced towards the shorter wavelengths.
This is a result of the panel becoming dirty from dust and sand in the field, as seen on the plastic case for the panel (in the lower right corner of B).
This change in reflectance demonstrates the necessity to clean, re-measure, or replace your panels when performing radiometric calibration.")
```

![`r fig_nums(name = "fig-calibration-panel")`](../../figs/micasense-rededge3-calibrated-reflectance-panel-deterioration.png)

### Georeferencing

It is important to consider how the geographic positions of objects within your UAS survey are important for your research.
Those positions can range from being globally accurate with precise correspondence to a location on the Earth (e.g., the tree is located at these coordinates, plus or minus 5 centimeters) to being relatively accurate with the spatial relationships and real-world distances between objects in the scene preserved but perhaps all frame-shifted by some amount compared to reality (e.g., the first tree is 5 meters away from the second tree, but all the trees are shifted 10 meters compared to their true on-the-ground coordinates).
In fact, it is possible for the SfM photogrammetry process to reconstruct 3D models and orthomosaics of the area of interest using visual cues in individual images alone without any geolocation data at all, resulting in a relative accuracy between objects in the scene but no ability to make real world measurements (e.g., the distance between the two trees is 5% of the map).
In order to infer units from these relative distances (e.g., to get the distance in meters), some measure of scale in the imagery is required.
Geolocating your SfM photogrammetry products in real-world space requires external information about the geolocation of each input image, such as from the Global Navigation Satellite System (GNSS).
Note that GNSS is the generic term for the network of satellites that offer global coverage of geospatial position, of which the US-owned Global Positioning System (GPS) is a part.
Most popular off-the-shelf vehicles and/or optical payloads have a basic GNSS antenna and receiver with an accuracy of <10 m, and the optical data collected will be automatically geotagged in the image metadata.
The automatic integration of these metadata in the most popular SfM photogrammetry software means that the second scenario described above-- relative spatial accuracy, but with SfM products frame-shifted by some amount similar in magnitude to the GNSS receiver accuracy-- is achievable with no extra steps by the user.
If you require greater accuracy than what is provided by the built-in GNSS receiver however, then additional steps are required.

Ground control points (GCPs), Real-time Kinematic (RTK) corrections, and Post-processed Kinematic (PPK) corrections are three solutions to accurately georeference images collected by the UAS.
GCPs are markers laid out on the ground with known geolocations that are visible in the UAS data and are used to tie your imagery to real-world coordinates during the SfM processing step.
The GCP approach can only be as precise as the tool used to measure the geolocations of the GCPs in the field.
To improve upon the geolocation accuracy already in place by using image metadata geotags from the basic GNSS receiver that is likely onboard the UAS, a high-precision GNSS must be used to mark the geolocations of the GPS.
A high-precision GNSS may be prohibitively expensive, but could potentially be borrowed or rented from geodetic services (e.g. nonprofit UNAVCO allows equipment to be borrowed for NSF-funded projects for free).
Ideally, GCPs will be placed near edges or randomly throughout the mission area, but the density of GCPs is typically more important, with [@santana2021] finding that 10 GCPs in their 2 ha area of interest were needed for sub-7 cm precision (but 4 GCPs produced 16 cm precision at all flight heights and GCP spatial distributions).
@zimmerman2020 found that it was optimal to place GCPs in the corners of the study site, as well as at low and high elevations within the study site.
GCPs must be visible from the sensor, so it’s best to place them in bright and open areas.
Finding suitable locations in heavily forested areas with closed canopies can be challenging, therefore, it may be beneficial to expand survey areas to include suitable areas for GCPs if none can be found within the area of scientific interest.
Examples of effective GCPs are fabric swaths placed in an X, bright-colored bucket lids, or checkered mats (`r fig_nums(name = "fig-gcps", display = "cite")`).
GCPs with more conspicuous, precise points make for more precise geolocating because that specific point can be more easily matched between the field- and UAS-measured data.
For instance, trying to identify the exact center of a bright-colored bucket lid from aerial imagery might allow for 10 cm of mismatch with the exact point measured on the ground, the intersection of two, 5-cm wide pieces of cloth might allow for 5 cm of mismatch, and the crisp intersection of the white and black triangles might only allow for 1 cm of mismatch (`r fig_nums(name = "fig-gcps", display = "cite")`).
Because the field measurements of GCP locations can be a slow step, it might be advantageous to install permanent monuments at desirable GCP locations, measure their precise locations once, then reuse those same points during future data collection (e.g., if not the conspicuous marker itself, perhaps a more discrete piece of rebar that can have the actual GCP draped over top of it just prior to new data collection).
Pre-existing permanent (or semi-permanent) points may also be used if they can be readily measured on the ground and are visible from the air.
For example, NEON TOS plots have permanent markers that have been georeferenced with high precision (approximately 0.3 m) that can be used as GCPs if they can be made visible from the UAS (`r fig_nums(name = "fig-photo-overlap", display = "cite")`).

RTK and PPK kinematic corrections augment the accuracy of a UAS' built-in GNSS receiver by correcting the noise inherent in the instrument using additional equipment and processing steps without the need for laying out GCPs and determining their locations.
This can result in massive time savings, particularly when surveying large areas.
For instance, @gillan2021 was able to survey and process data covering over 190 hectares of rangeland in approximately 30 days versus an estimated 141 days using a conventional UAS workflow, with an estimated 47 days saved just from using an RTK system versus GCPs.
Even with RTK and PPK corrections, it is still considered good practice to lay out some GCPs at precisely known locations, then quantify geolocation error in the final SfM products by measuring the difference between the field- and UAS-measured GCP locations.


```{r}
fig_nums(name = "fig-gcps", "Aerial RGB photograph captured using a DJI Phantom 4 Pro on January 23, 2020 at 120m of altitude above ground level.
The photo depicts three ground control points (GCPs) each of two different types in the center of the image: 1 m long spray painted orange cotton drop cloth in an “X” pattern and 1 x 1 m squares of cotton drop cloth spray painted with black triangles.
The GCPs are progressively more conspicuous under the canopy, in the shrub field, and on the dirt road.
The size of the area covered by the main photograph is approximately 180 m wide x 120 m high.")
```

![`r fig_nums(name = "fig-gcps")`](../../figs/example-gcps-with-inset.png)

### Data management

Data management can be broken into two phases: first, post-collection, while you’re processing the data and need easy access to them, and second, the long-term storage of the data and metadata.
It’s important to bring empty Secure Digital memory cards (SD cards) with fast write speed into the field to swap out of the UAS for frequent data transfer, as well as a laptop and an external hard drive(s) to transfer the data immediately, and back them up.
Have an idea of your anticipated data storage requirements for each mission to budget hardware accordingly (`r fig_nums(name = "fig-data-management-schematic", display = "cite")`).
Like with data processing, one must consider whether to store data locally or in the cloud, and how to backup the data.
For long-term storage, you will want to consider which data products from the mission to publish and how you’ll want to make these products publicly available.
We suggest publishing the raw images taken from UAS missions which may be processed to even higher quality products given the rapid advances in structure from motion photogrammetry.
You’ll also want to figure out good long-term storage for the raw data at your institution or elsewhere. 
For such high volumes of data, it is sound data management practice to establish "data levels" that characterize how derived each new processed product is, with Level 0 representing raw data and higher levels being derived from lower levels [@wyngaard2019].

For data collection, we recorded each flight’s imagery on a separate 32 GB SD card rated at >90 MB/s write speed.
For multi-day trips or if SD cards need to be reused, we transfer imagery from the SD cards to a portable solid state hard drive (Samsung T series).
 Upon returning from the field, we transferred images from the SD cards (or portable solid state hard drive, as the case may be) to two locations: 1) the solid state hard drive on a local desktop gaming computer for short term storage, and 2) a Network Attached Storage (NAS) with 6 spinning disk hard drives in a RAID array for long-term storage.
Both the short-term storage (local desktop) and long-term storage (NAS) solutions are backed up to the cloud using a 3rd party backup client (Backblaze) at a cost of ~$5.00 USD per terabyte per month.
We used the local desktop (Alienware Aurora R7 with an Intel i7-8700k 3.70 GHz hexacore processor and 64 GB of RAM) for data processing.
We use the same data levels as @koontz2021, except we didn't process our data to Level 3b or Level 4.
We used the Open Science Framework for public-facing storage (available upon publication).

```{r}
fig_nums(name = "fig-data-management-schematic", "Planning a data management pipeline is a large upfront investment but can save time and money in the long-run, making it well-worth prioritizing up-front.
Considering the storage, backup, and sharing needs of the datasets you anticipate collecting and processing ensure data persistency and availability.
This data pipeline describes options and includes recommendations.
There are trade-offs at each decision point, so it is important to understand your data needs and budget.
For example, whereas building your own data management system may be more affordable and tailored to your needs, rented external storage systems will back up your data automatically and maintain the system and hardware requirements for you.")
```

![`r fig_nums(name = "fig-data-management-schematic")`](../../figs/data-management-schematic.png)

### Data processing

It’s recommended to perform quality assurance (QA) checks on the images while you’re in the field, as a reality check before you leave and start processing (`r fig_nums(name = "fig-data-management-schematic", display = "cite")`).
This could mean viewing the images on a laptop on-site, or while still on location near the field study site.
Check the data for obvious artifacts such as over or under exposure in images, that the number of images expected were collected, and that file sizes appear consistent and reasonable.
Generally a full QA assessment cannot be performed in the field due to time and computation limitations, but the field QA should be sufficient to ensure the images can be processed into desired products.
Some NEON sites (e.g., NIWO) have a field house that may be accessed, with permission, for laptop friendly workspaces and / or charging options.
Contact the site host at each site for more information.
Processing UAS imagery to create an orthomosaic or point cloud can be CPU, RAM, and GPU-intensive, so a workstation that balances these hardware components is ideal.
One may have a workstation powerful enough to process images locally, or they may want to invest in a cloud-processing option [e.g., Cyverse-- see @swetnam2018] (`r fig_nums(name = "fig-data-management-schematic", display = "cite")`).
Be sure to be consistent with the datum/projection for each of your datasets (GNSS positions of GCPs, GNSS locations of UAS camera), and transform where necessary.

Many software applications are available for structure from motion photogrammetry processing that produce results of similar quality [@forsmoo2019].
You must select the appropriate post-processing (i.e., photogrammetry) software license for you needs and resources (e.g., Agisoft Metashape, Pix4D), and take note of what processing steps you take from raw data to final products, for sake of reproducibility.

We followed the USGS workflow to process our raw Micasense RedEdge 3 imagery into an orthomosaic and a dense point cloud (https://uas.usgs.gov/nupo/pdf/USGSAgisoftPhotoScanWorkflow.pdf).
We classified the dense point cloud into "ground" and "non-ground" points using a cloth simulation filter algorithm [@zhang2016] implemented in the `lidR` [@roussel2019] package.
We used a local maximum filter to detect individual trees [@popescu2004] as implemented in the `lidR` package and a marker controlled watershed segmentation algorithm to segment individual tree crowns [@plowright2018].
We used the `hsdar` package to spectrally resample the NEON imaging spectrometer data to match the spectral resolution of the Micasense RedEdge 3 camera [@lehnert2019].

<!-- ```{r} -->
<!-- fig_nums(name = "fig-spectral-overlap", "Relative spectral responses for two channels of the Micasense RedEdge 3 camera plotted with the relative spectral responses for twenty channels of the NEON AOP imaging spectrometer. -->
<!-- Several channels of the NEON AOP instrument comprise each of the Micasense RedEdge 3 channels, so the reflectance data from the NEON AOP are resampled (weighted, in effect) such that they can be used as though the NEON instrument exhibited the same spectral sensitivity as the Micasense RedEdge 3 instrument.") -->
<!-- ``` -->

<!-- ![`r fig_nums(name = "fig-spectral-overlap")`](../../figs/relative-spectral-response_micasense-rededge3-vs-neon-aop.png) -->

```{r}
fig_nums(name = "fig-comparing-scenes", "NDVI derived from the NEON AOP spectral imager using data collected in August of 2019 (data collection flights over NIWO on August 14, 15, 19, and 26 of 2019) and the Micasense RedEdge 3 camera using data collected on October 09, 2019.
The NEON AOP data were first spectrally resampled into the equivalent red and near infrared bands of the Micasense RedEdge 3 camera based on the relative spectral response of the RedEdge 3 instrument.
The higher spatial resolution of the drone-derived data are highlighted.
Note that the difference in NDVI between the images may derive from three main sources: phenological differences in the vegetation, differences in the flight conditions such as time of day and cloud cover, or differences in instrumentation.")
```

![`r fig_nums(name = "fig-comparing-scenes")`](../../figs/ndvi_neon-spectral-resampled-v-drone-original.png)

## Case study

### Science Requirements
Forest inventories describe the geolocation and physical attributes of individual trees, and provide critical information for management decision-making and advancing ecological theory [@young2021].
Remote sensing approaches to creating forest inventories can cover more area than field-based methods at a lower cost per area, and recent approaches still allow for characterization of individual trees [@weinstein2019].
NEON collects field-based forest inventory data in their Terrestrial Observation System (TOS) (the "Woody Plant Vegetation Structure" data product; DP1.10098.001) and remote sensing data in their Aerial Observation Platform that have been used to generate forest inventory data [@weinstein2020].
The field-based data are restricted to 20 x 20m field plots, while the AOP data cover dozens of square kilometers but at moderately coarse resolution (10 cm for RGB imagery, 1 m for imaging spectrometer data, 1 m for LiDAR data).
UAS have the capacity to fill in missing scales of observation for creating forest inventories by capturing a broader spatial extent than field-based NEON data, but at a finer spatial resolution than NEON AOP data.
With this as a motivation, here we present a case study where we collect and process UAS data coincident with a NEON TOS plot to create a forest inventory.
We use the previous section's "core considerations" as a framework for describing our workflow, and provide all data and code to further aid our mental model building. 

### Vehicle

We flew a DJI Matrice 100 vehicle, which is a rotocopter style drone with four propellers and a proven track record of safe, predictable flights.
The vertical take offs and landings of the rotocopter style allowed us to operate the vehicle from a clearing as small as the width of the dirt access road to the site.
The Matrice 100 has a relatively high lift capacity that allows for a payload to be integrated, and is heavier than many consumer rotocopters which makes it both more stable in windy conditions and more challenging to transport beyond a road.

### Payload
We used a Micasense RedEdge3 sensor, which captures reflectance information in 5 distinct channels.
Using the quantum efficiency and filter bandpass sensitivity of an average RedEdge3 sensor provided by Micasense, we can estimate the relative spectral response of the instrument which characterizes how the sensor captures reflectance information across the electromagnetic spectrum (`r fig_nums(name = "fig-rededge-rsr-neon-aop", display = "cite")`).

### Environment
Our data collection took place on a single day under mostly sunny, light wind conditions on October 9th, 2019 starting at 2pm mountain daylight time.
We ideally would have flown closer to solar noon to minimize shadows in the imagery, particularly this late in the year.

### Flight Planning
We used 

### Regulations

We obtained permission to access the NIWO NEON site from the site host, the Mountain Research Station as well as NEON itself.
The airspace over NIWO is Class G uncontrolled up to 1,200 feet above ground level.

### Radiometric Calibration

### Georeferencing

We laid out orange cloth X's over the 9 permanent markers within the NIWO_017 field site.
Five of these points ended up being visible from the air.
These ground control points were located within the center of the flight area, without any geolocation representation at the edges which wasn't ideal.

### Data Collection

### Data Processing


## Discussion

Macroecology will benefit from a “macroscope” to enable the study of broad-extent phenomena across multiple scales of biological, geophysical, and social processes [@beck2012; @lawton1996; @dornelas2019]. 
The ideal macroscope comprises a nested array of tools that provide full coverage of spatial and temporal observational domains. 
In their complementarity, the value of multiple observational tools in concert is more than the sum of the parts [@dornelas2019]. 
Pairing UAS with NEON partially completes the constellation of Earth observing tools that contribute to the macroscope, and combines the flexiblity of UAS 
In this work, we aid the adoption of these tools amongst macroecologists by providing a mental model and some practical considerations for their integration.

Challenges remain for integrating UAS with NEON, but they are surmountable.
Some of these challenges are fundamentally associated with "big", cross-scale data.
Integrating data cross scales brings a host of potential pitfalls that could pollute inference if care isn't taken to avoid them [@zipkin2021].
Clear documentation of data provenance including sensor characteristics, data acquisition methods (e.g., flight pattern), data acquisition conditions (e.g., time of day, cloud cover) will enable rigorous cross-scale data integration.
The proliferation of reasonably low-cost, off-the-shelf, drone-ready sensors (many designed for precision agriculture use) creates a need for validation of whether those instruments produce "science grade" data (which itself is a relative term, depending on what the specific science requirements are for a given project).
This validation may be achieved via direct comparison of the low-cost sensors with “state of the science” instruments using coincident flights [e.g., @fawcett2020].
Big data in ecology is relatively new [@farley2018], and approaches to UAS-derived big data are fairly ad hoc across researchers [@wyngaard2019].
Maintaining supportive communities of practice, such as the High Latitude Drone Ecology Network (https://arcticdrones.org/), can help overcome some of these idiosyncratic approaches.
Integrating UAS operations with NEON can help anchor the community to the common currency of NEON data types, organization, and collection protocols which will enhance the interoperability of UAS data.
Cyberinfrastructure for managing and processing UAS data are not yet built in a way that encourages consistency between projects or researchers.
In this way, NEON provides an aspirational example for how purpose-built cyberinfrastructure can facilitate macroecology, and the foundational resources for building an equivalently valuable architecture for UAS data may already be represented in other NSF-sponsored projects (e.g., Cyverse, OpenTopography, Open Science Framework). 
NEON also provides an aspirational target for educational resources, which are critical to ensuring that would-be NEON/UAS users have the environmental data science skills necessary to turn their data into inference [@hampton2017].
Critically, increasing access to an elusive observational domain (broad-extent, fine-grain) doesn’t equate to it being fully accessible. 
We can illustrate this point using access to the Landsat archive, which we previously discussed as an exemplar of the effect of data access on democratization of science. 
While the reduced cost of Landsat images brought more researchers from lower prestige institutions and lower income parts of the world into the user base [@nagaraj2020], those users are still overwhelmingly men [@miller2016; @miller2016a].
A truly accessible tool wouldn't exhibit such a demographic bias, and it is incumbent upon the UAS and NEON community to be self-reflective about who is being excluded from the community and proactive about what steps can be taken to make it more inclusive.

We conclude with a research agenda for UAS/NEON integration, which we hope provides a vision to be built upon. Integrating UAS with NEON allows:

1. Filling in spatial and temporal scales missed by NEON data collection (e.g., capturing data in a year when a NEON site is skipped by the AOP, collecting data on a similar vegetation type of a NEON site but outside of NEON's direct footprint, capturing data at a site multiple times per year to understand phenomena at sub-yearly time scales, tracking individual plants through time, capturing data at spatial resolutions finer than 10 cm)
1. Opportunistic data collection (e.g., capturing data immediately after a disturbance event)
1. Connecting NEON data to other Earth observing systems using UAS data as a bridge
1. Supplementing NEON data using sensors that aren't part of the NEON suite of sensors (e.g., thermal data to compare thermal regulation of different plant species) 
1. Validating lower cost, off-the-shelf payloads against the state-of-the-science NEON data collection (e.g., determining how well a multispectral imager designed for agriculture captures surface reflectance, determining how well an algorithm detects the trees in a NEON vegetation structure plot)
1. Replacing high-cost NEON AOP flights with lower-cost alternatives (e.g., if the drone-derived data are "good enough" compared to the AOP, can we reduce the operational costs of the AOP)
1. Using NEON data as a common currency for validating new methods 

## Acknowledgements

Funding for the 2019 NEON Science Summit was provided by NSF Award #1906144. Additional funding was provided by Earth Lab through the University of Colorado, Boulder’s Grand Challenge Initiative, the Cooperative Institute for Research in Environmental Sciences, and the North Central Climate Adaptation Science Center. We would also like to acknowledge the National Aeronautics and Space Administration (NASA) New Investigator Program (NIP) grant 80NSSC18K0750 to M. E. Cattau. The National Ecological Observatory Network is a program sponsored by the National Science Foundation and operated under cooperative agreement by Battelle. This material is based in part upon work supported by the National Science Foundation through the NEON Program.

## Data availability
All data and code required to reproduce the analysis and the workflow presented in this manuscript can be found on the Open Science Framework at (available upon publication) and on GitHub (available upon publication)

<!-- ## Author contributions -->

<!-- Author contributions are defined using the Contributor Roles Taxonomy (CRediT; https://casrai.org/credit/). -->
<!-- Conceptualization: ; Data curation: ; Formal analysis: ; Funding acquisition: ; Investigation: ; Methodology: ; Project administration: ; Resources: ; Software: ; Supervision: ; Validation: ; Visualization: ; Writing -- original draft: ; Writing -- review and editing:    -->

\newpage

## References