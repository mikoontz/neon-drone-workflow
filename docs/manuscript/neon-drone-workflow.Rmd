---
bibliography: ../neon-drone-workflow.bib
csl: ../ecosphere.csl
params:
  title: "Democratizing macroecology: integrating uncrewed aerial systems with the National Ecological Observatory Network"
  author: |
    Michael J. Koontz^1\*^,
    Victoria M. Scholl^1,2,3^,
    Anna I. Spiers^1,4^,
    Megan E. Cattau^5^,
    John Adler^2,6^,
    Joseph McGlinchy^1,7^,
    Tristan Goulden^6^,
    Brett A. Melbourne^4^,
    Jennifer K. Balch^1,2^
  affiliation: |
    ^1^Earth Lab, University of Colorado; Boulder, CO, USA  
    ^2^Department of Geography, University of Colorado; Boulder, CO, USA  
    ^3^Geosciences and Environmental Change Science Center, U.S. Geological Survey; Lakewood, CO, USA  
    ^4^Department of Ecology and Evolutionary Biology, University of Colorado; Boulder, CO, USA  
    ^5^Department of Human-Environment Systems, Boise State University; Boise, ID, USA  
    ^6^Battelle, National Ecological Observatory Network (NEON); Boulder, CO, USA  
    ^7^Hydrosat Inc., Washington, DC 20036, USA

  correspondence: |
    mikoontz@gmail.com
  keywords: |
    NEON, macroecology, UAS, drone    
  word_count: |
    abstract (260); intro (1605); core principles (866+1968+726+989+1026+813+656); case study (933+169+138+980+236+399); discussion (909) = 12413
  date_generated: !r format(Sys.Date(), "%B %d, %Y")
  
geometry: margin=1in
header-includes:
  - \usepackage[left]{lineno}
  - \linenumbers
  - \usepackage{setspace}
  - \doublespacing
  - \DeclareUnicodeCharacter{200E}{}
  - \usepackage{caption}
  - \captionsetup[figure]{labelformat=empty}
  - \captionsetup[table]{labelformat=empty}
  - \usepackage{amsmath}

output:
  # pdf_document
  bookdown::word_document2
  # bookdown::html_document2: default

---

```{r setup, include = FALSE, echo = FALSE}
knitr::opts_chunk$set(echo = FALSE, include = FALSE)
```

# `r params$title` {-}

`r params$author`

`r params$affiliation`

*^\*^Correspondence:* ``r params$correspondence``

*Keywords:* `r params$keywords`

Date report generated: `r params$date_generated`

Open Research Statement: All data and code required to reproduce the analysis and the workflow presented in this manuscript can be found on the Open Science Framework at https://doi.org/10.17605/OSF.IO/ENBWU and on GitHub at https://github.com/mikoontz/neon-drone-workflow.

```{r libraries}
library(tidyverse)
library(pander)
library(captioner)
```

```{r captions}
fig_nums <- captioner(prefix = "Figure")
table_nums <- captioner(prefix = "Table")
eq_nums <- captioner(prefix = "Equation")
```

# Abstract 

Macroecology research seeks to understand ecological phenomena with causes and consequences that accumulate, interact, and emerge across scales spanning several orders of magnitude.
Broad-extent, fine-grain (i.e., high spatial resolution) information is needed to adequately capture these cross-scale phenomena, but these data are costly to acquire and process. 
Uncrewed aerial systems (UAS, or drones carrying a sensor payload) and the National Ecological Observatory Network (NEON) make the broad-extent, fine-grain observational domain more accessible to researchers by lowering costs and reducing the need for highly specialized equipment.
Integration of these tools can further democratize macroecological research, as their strengths and weaknesses are complementary.
However, using these tools for macroecology can be challenging because mental models are lacking, thus requiring large upfront investments in time, energy, and creativity to become proficient. 
This challenge inspired a working group of UAS-using academic ecologists, NEON professionals, imaging scientists, remote sensing specialists, and aeronautical engineers at the 2019 NEON Science Summit in Boulder, CO to synthesize current knowledge on how to use UAS with NEON into a mental model for an intended audience of ecologists new to these tools. 
Specifically, we provide (1) a collection of core principles for collecting high-quality UAS data for NEON integration and (2) a case study illustrating a sample workflow for processing UAS data into meaningful ecological information and integrating it with NEON data collected on the ground with the Terrestrial Observation System and remotely from the Airborne Observation Platform. 
With this mental model, we advance the democratization of macroecology by making a key observational domain-- broad-extent, fine-grain data-- more accessible via NEON/UAS integration.

# Introduction

Macroecology is the study of spatially extensive systems whose biological, geophysical, and social components interact dynamically both within and across spatiotemporal scales [@heffernan2014].
Macroecology, in its explicit consideration of scale, extends from a rich history of basic ecological research seeking to explain patterns in nature [@turner1989; @levin1992]. 
At the same time, macroecology is highly relevant to applied ecology, as the broader spatial extents studied reflect the scale at which many societally relevant challenges, and perhaps their solutions, arise [@heffernan2014; @larue2021]. 
The causes and consequences of phenomena under investigation in macroecology can span many spatial scales, which motivates a characteristic feature of the data to be brought to bear: they must often be simultaneously fine in grain (i.e., spatial resolution) and broad in extent [@beck2012].

Ecologists typically face a data collection tradeoff between grain and extent that constrains the observational domain of their research [@ernest2018; @estes2018]. 
Indeed, the spatial and temporal observational domains of most ecology research is narrow [@estes2018].
The grain/extent tradeoff can sometimes be overcome, but at a high cost.
For example, the Global Airborne Observatory collects high spatial and spectral resolution data at broad extents [@asner2007; @asner2012], but the price of data acquisition and processing tallies in the millions of US dollars, even though the per-area cost is low [@asner2013].
As another example, the US Forest Service Forest Inventory and Analysis program maintains a regular network of over 350,000 fine-grain field plots regularly spaced over the entire forested area of the United States (over 9.1 million km^2^; approximately 1 plot every 2400 hectares) at an annual cost of tens of millions of dollars [@gillespie1999; @alvarez2020].
Most science studies have relatively modest budgets and are conducted by just a few individuals [@heidorn2008].
The modal award size from the National Science Foundation's (NSF) Division of Environmental Biology was about $200,000 USD between 2005 and 2010 [@hampton2013].
While the fine-grain, broad-extent observational domain is invaluable for macroecology, it can be inaccessible to ecologists with resource or funding limitations.

Macroecology can be democratized when barriers to research participation are reduced [@guston2004], such as by innovating past limitations to or lowering the cost of access to relevant scales of observation.
Removing these barriers improves science because the rate, direction, and quality of science are, in part, shaped by the available research inputs [@nagaraj2020].
For instance, the cost of imagery from the archive of Landsat earth observation imagery was reduced in 1995 and restrictions on sharing were relaxed, which dramatically increased the quantity, quality, and diversity of Landsat-enabled science [@nagaraj2020].
The same archive became freely available in 2008 with concomitant benefits to projects that rely on Landsat observations [e.g., @picotte2020].
The changing accessibility of Landsat data is noteworthy for macroecology, as the archive provides consistent global-extent, relatively fine-grain (30 meter) imagery since 1984.
Particularly when integrated with other tools whose purpose is to broaden research participation [such as the free, planetary-scale geographic information system "for everyone," Google Earth Engine\; @gorelick2017], Landsat imagery has led to breakthrough science that is “globally consistent and locally relevant” such as the first global map of forest cover changes over a decade-long period at a relatively fine scale [@hansen2013]. 
Thus, democratized research stimulates revolutionary science.

In its democratic aim, the National Ecological Observatory Network (NEON) is revolutionary [@nsf2013; @balch2020].
NEON is a continental-scale observation facility in the United States comprising 81 sites within 20 ecoclimatically distinct domains and an operational lifespan on the order of decades [@schimel2013; @keller2008].
NEON is designed to collect rigorous, consistent, long-term, and open access data to better understand how US ecosystems are changing, using a combination of field measurements by trained personnel, ground- and aquatic-based automated sensors, and plane-based instruments that collect both active and passive remotely-sensed data [@kampe2010; @nsf2013].
NEON observations span spatial scales, from measurements of individual organisms within small field plots to 10 cm resolution red-green-blue (RGB) imagery and 1 m hyperspectral and lidar imagery across hundreds of square kilometers, with measurements replicated across  sites that span the  continental extent of the Observatory [@keller2008].
A stated goal of NEON is to democratize access to ecological research, particularly at broad extents [@nsf2013]-- its promise is continental-scale ecology for everyone.
NEON pairs publicly available data with a strong outreach and education effort to help realize this promise.
In this way, NEON broadens access to macroecology by reducing barriers to entry, particularly cost, fieldwork requirements, and technical expertise [@nagy2021].
An “instrument” like NEON collecting standardized data at such scales leads to inevitable tradeoffs-- in the specific times, locations, and type of data that are sampled.
While the NEON data are on their own sufficient for advancing ecology, part of what makes NEON revolutionary is its foresight in facilitating connections to other ecological data.
In this way, the fundamental limitations of NEON can be overcome with bridges to more targeted ecological studies.

Uncrewed aerial systems (UAS) can also revolutionize ecology [@anderson2013b].
UAS, comprising a vehicle and a payload, are increasingly being used to collect high spatial resolution information over relatively large spatial extents for ecological science applications [@wyngaard2019].
The vehicle is also known as a “drone” or a “UAV” standing for “uncrewed aerial vehicle”, “unhumanned aerial vehicle”, “unoccupied aerial vehicle”, or “unmanned aerial vehicle”, though we support phasing out the gendered language of this last expansion [@joyce2021].
The payload is the instrumentation carried by the vehicle beyond what is critical for flight operations, and gives the UAS its scientific value.
Importantly, it isn’t the vehicle itself that enables ecological studies at heretofore inaccessible scales, but rather the vehicle’s ability to position a data collecting payload (i.e., a sensor) in a repeatable, efficient, hard-to-reach manner.
For example, one use case for UAS is structure from motion (SfM) photogrammetry which generates a 3-dimensional model of an area of interest using 2-dimensional images from multiple overlapping viewing angles [@westoby2012].
The minimum requirement for SfM photogrammetry is 2-dimensional imagery, which can be captured from the ground using a hand-held sensor (e.g., a digital camera) to great effect for some applications [@piermattei2019].
A UAS-based camera can capture imagery from higher up in, or above, the canopy, which allows for measurement of higher vegetation strata [@kuzelka2018], including total height for above-canopy applications.
UAS-based SfM photogrammetry also increases the extent that can be covered with surveys [@jackson2020], since aerial transects are unimpeded by varied terrain and vegetation encountered on ground transects.
Unimpeded aerial transects are also more reliably repeated than ground surveys that require navigating through vegetation and are likely to be less impactful to that vegetation.
UAS provide an avenue to flexibly and affordably fill spatiotemporal gaps in data collected by traditional means-- they can be deployed more frequently and capture finer grain data than airplane- and satellite-based platforms, and can cover greater extents than ground surveys.

UAS and NEON complement each other.
Each can be a key tool for macroecology research, but their integration offers an opportunity to alleviate some of their fundamental constraints in a similar way as integration of NEON with other earth observing networks [@balch2020; @nagy2021].
NEON data derive from "state of the science" instrumentation with thorough documentation, and are standardized at a continental scale.
NEON data collection is pre-planned, which makes the resulting data somewhat predictable, but also rigid in space, time, and type.
On the other hand, UAS operations are nimble and customizable, but the resulting data are relatively under-validated with data standards that are ad-hoc, idiosyncratic, and lacking in a consistency which makes interoperability of those data across projects a challenge [@wyngaard2019].
Realization of the benefits of UAS-NEON integration by ecologists is dually challenged by the relative novelty of these tools [@wyngaard2019; @nagy2021] as well as by a community gap in the data science skills needed to navigate their associated workflows [@hampton2017; @balch2020; @nagy2021].
Not knowing where to start with two new tools is a daunting proposition, and unstructured efforts to gain practical proficiency for research often comes at the expense of doing research itself [@olah2017].
Reducing these barriers to proficiency therefore has tremendous research value.

Mental models help novices become experienced practitioners by providing a contextual framework for new knowledge [@knapp2010].
A lack of a synthesized contextual framework for practical use of UAS for ecology research, particularly for NEON integration, challenges adoption of these tools and hampers their ability to democratize macroecology [@assmann2019; @wyngaard2019].
We assembled a working group of participants at the 2019 NEON Science Summit in Boulder, Colorado with a goal to synthesize current practical knowledge and provide a sample workflow to guide ecologists with a mental model for using UAS and integrating with NEON. 
In this work, we aim to lower the barrier to entry for using UAS and NEON to do ecology.
Specifically, we focus on optical data collected by each tool over terrestrial sites and provide (1) a collection of what we consider to be the ten core principles for integrating UAS with NEON (science requirements, vehicle, payload, environment, flight planning, rules/regulations, radiometric calibration, georeferencing, data management, and data processing) and (2) an illustration of these principles with a real-world, well-documented workflow that processes UAS data into meaningful ecological information, then integrates it with NEON Airborne Observation Platform (AOP) and Terrestrial Observation System (TOS) data at the NEON Niwot Ridge (NIWO) site.

# Core principles for UAS/NEON integration

## Science requirements

Both NEON- and UAS-enabled ecology have faced criticism for having “backwards” approaches to science.
Critics suggest that clear science questions take a back seat to high-quality but rote data collection in the case of NEON [@lindenmayer2018] or perpetual methodological refinement in the case of UAS [@gillan2021].
In light of this, we echo @assmann2019 in emphasizing that the main consideration to keep in mind while integrating UAS with NEON is: what are the science requirements for the data to be collected and what data collection efforts are "good enough" to meet those requirements?
A clear science question helps guide the data collection needs, which can minimize the amount of researcher time and energy spent on developing tools that ultimately prove to be superfluous [@joseph2021].

## Vehicle

The vehicle in a UAS is the flying machine that holds the payload.
One key distinction between vehicle types is whether rotor systems or fixed wings are used for lift (the upward force that keeps the vehicle in the air).
Rotocopter vehicles (also known as “multicopters”, “multirotors”, “quadcopters”, “hexacopters”, or “octocopters” depending on the number of rotor systems) consist of a body and 4-8 rotary systems that provide both lift and thrust (horizontal motion).
These types of vehicles are characterized as “vertical takeoff and landing” (VTOL).
Fixed wing aircraft use wings for lift, and use rotor systems only for thrust.
Hybrid vehicles use rotor systems for lift during ascent and descent, but fixed wings for lift during flight and are sometimes referred to as VTOL fixed wing systems to highlight this combination of features.
The structure and size of the vehicle determine its functionality in the field and thus a project’s objectives can often help constrain the choices available.
Rotocopter platforms are more maneuverable, often less expensive, easier to fly, more transportable, and have a higher payload capacity relative to fixed wing aircraft.
For these reasons, rotocopters are often preferred by ecologists.
On the other hand, fixed wing aircraft have longer flight times with better battery usage and thus can cover larger areas more efficiently than rotocopters.
For example, covering the full extent of a given AOP footprint (147.6 +/- 107.2  km^2^ for core and relocatable sites) may be most efficiently conducted with a fixed-wing or hybrid vehicle.
They are also more stable in adverse conditions (e.g., high winds) and have a safer recovery from motor power loss.
VTOL fixed wing systems can combine the efficiency of a fixed wing with the small takeoff/landing footprint of a rotocopter.
A summary of the advantages and disadvantages of these vehicle types is found in `r table_nums(name = "table-vehicle-payload", display = "cite")`.

A flat surface clear of obstructions (e.g., on dirt rather than grass, away from forest canopy) is ideal for UAS takeoffs and landings.
VTOL systems require a smaller takeoff and landing footprint, which may be satisfied with only a small canopy gap, compared to vehicles that use fixed wings for lift, which require a “runway” for fixed wing vehicles.
Locating such a site may be challenging at some NEON sites (e.g., NIWO, with dense canopy cover) and easy at others (e.g., SJER, with an open woodland ecotype).
Takeoffs and landings from a clean, stable, flat surface (e.g., plywood, car floor mat) will prevent dirt from obstructing or scratching the sensor lens, and will make for a more controlled ascent/descent.

With any platform, vehicle endurance limitations and the mission goals will determine how many flights are required to complete data collection.
In many cases, it will be necessary to purchase several batteries to keep the vehicle flying for the duration of a field day.
Even if only one flight is needed to collect data, extra batteries are still valuable to have on hand in case the first flight doesn't go as planned and follow-up flights are required.
An energy source to charge batteries in the field, like a solar charger or gasoline-powered generator, may also be necessary for very long missions or multiple days of data collection.
As a guideline, you can determine how many batteries your portable energy source can charge by determining its energy capacity in watt-hours (Wh), multiplying by 90% (making the calculation such that you leave 10% of the capacity rather than fully draining it), then dividing by the capacity of a single UAS battery in watt-hours and rounding down to the nearest whole number to account for any unpredictable inefficiencies.
For instance, a Goal Zero Yeti 1400 battery (used successfully by some authors) can be charged with solar panels and stores 1,400 Wh of energy, which results in 1,260 Wh of usable energy if it were to be drained to 10% capacity.
Each TB48 battery of the DJI Matrice 100 aircraft stores about 130 Wh of energy, so the Yeti 1400 should be able to charge about nine batteries before it needs to be recharged itself (1,260 / 130 = 9.7, which is 9 when rounded down).
If an efficient one-gallon, gasoline-powered generator can produce 6,000 Wh of energy, that results in 5,400 Wh of usable energy which is equivalent to charging about 41 batteries for the Matrice 100.

```{r}
table_nums(name = "table-vehicle-payload", "Summary of vehicle and payload considerations for UAS-enabled ecology.")
```


```{r vehicle_payload_table_print, echo = FALSE, include = TRUE, message = FALSE, results = 'asis'}
vehicle_payload_table_print <- readr::read_csv("../../tables/table1-vehicle-payload-considerations.csv",
                                               na = character())

pander::pandoc.table(vehicle_payload_table_print,
                     split.tables = Inf,
                     caption = table_nums(name = "table-vehicle-payload"),
                     keep.line.breaks = TRUE)
```

## Payload

The payload is the equipment carried by the UAS that collects data and combines with the vehicle to constitute the system (the "S" in "UAS").
In fact, despite the typical focus on the drone vehicle, the payload component is at least as important, since the main purpose of the vehicle is merely to position the payload where it needs to be in order to capture appropriate data.
For ecologists interested in optical data, the payload may be a simple camera or a more specialized remote sensing sensor sensitive to particular wavelengths of electromagnetic radiation.
The scientific questions will dictate the data requirements, which will in turn drive the payload decision.
Typically, the selection of a sensor represents a trade-off between spatial resolution (the size of pixels in the imagery at a set altitude), spectral resolution (the number of distinct portions of the electromagnetic spectrum that the sensor can detect), spectral extent (how much of the electromagnetic spectrum the sensor can detect), and cost.
For example, while hyperspectral data provide high spectral resolution and extent that may allow measurement of specific chemical compounds in vegetation [e.g., foliar nitrogen\; @knyazikhin2013], a multispectral instrument with fewer spectral channels [@koontz2021] or even an RGB camera [@scholl2020] may be more than sufficient for classifying vegetation to species.
Similarly, sensors with high spatial resolution can capture fine detail in their imagery but may reduce the ability to measure a variable of interest, such as individual trees, as post-processing steps can be negatively affected by the movement of those fine details in the wind [@young2021].
Hyperspectral instruments and high resolution cameras are relatively expensive in terms of purchase cost, post-processing time, and data storage requirements, but simple RGB and multispectral cameras can be affordably bought off-the-shelf so it is worth considering whether they would suffice for the scientific question of interest.
A summary of the advantages and disadvantages of these different payload types for collecting optical data can be found in `r table_nums(name = "table-vehicle-payload", display = "cite")`.

It is also important to consider how the payload will be integrated with the vehicle, which generally requires considering the combination of the vehicle and payload simultaneously.
In some cases, the payload can operate entirely independently from the vehicle, and integration only requires a means of physically attaching the components together.
In other cases, the payload relies both on power and electronic signaling from the vehicle in order to capture data, and integration may require more specialized electrical and mechanical engineering expertise.
It's generally advisable to use a pre-built integration kit or an already-integrated sensor/vehicle system if the payload meets the science requirements (or nearly so).

## Environment

The environment of the UAS mission can affect both the equipment performance and the data collection such that the intended operation conditions must be considered during vehicle/payload selection and flight planning.
Foremost, the vehicle and the payload must be capable of functioning in the desired environment.
UAS flights at high elevations or in cold weather will drain battery faster than at sea level, and some popular vehicles won't allow takeoff if the temperature is too cold (or hot).
While some vehicles are designed to withstand light precipitation and dust, many would be damaged under such flight conditions.
Heavy winds can push the UAS off-course or require the UAS to work harder to maintain its course, which drains battery faster and reduces endurance.
Variable terrain within the survey area may also affect vehicle endurance, as more energy is required to ascend and descend while also traversing along flight transects in the horizontal plane.
Managing the temperature of the mission critical electronics is just as important as that of the vehicle’s batteries during UAS operations.
The vehicle remote controller and any other peripherals such as a tablet computer are susceptible to battery drain in extreme temperatures, and cold temperature can cause the vehicle and/or sensor to malfunction.
The mean annual temperature for NEON AOP sites ranges from -12°C at the Utqiaġvik site in Alaska to 25°C at Lajas Experimental Station in Puerto Rico (NEON Field Site Metadata; https://www.neonscience.org/sites/default/files/NEON_Field_Site_Metadata_20210226_0.csv; accessed 2021-03-16).
Expectations of unfavorable environmental conditions may be enough to dictate what equipment should comprise the UAS.
For example, high-wind conditions at NIWO may warrant a fixed wing platform; however, the dense forest would make takeoff and landing much easier with a rotocopter.
In some cases, steps can be taken to mitigate the unfavorable environmental conditions, such as keeping equipment out of direct sunlight to prevent overheating (to the point of adding sun umbrellas or shade tarps to the required equipment list) and storing batteries in a dry cooler when not in use in order to insulate them against temperature extremes.

Environmental conditions may also impact data collection on automated flights, particularly for optical data.
Ideal conditions for optical data collection are evenly lit with either complete cloud cover or clear skies.
If flying takes place under clear sky conditions, then the sun should be high in the sky so it doesn’t cast long shadows-- ideally within a couple of hours of solar noon (i.e., 10am and 2pm for standard time, and 11am to 3pm for regions that observe daylight savings time) [@assmann2019].
Note that some SfM software guidelines specifically suggest *not* flying near solar noon, as this can create particularly bright areas within each image that challenges the SfM algorithms (MapsMadeEasy; https://www.mapsmadeeasy.com/data_collection; accessed 2021-11-19).

Prior to flights, it is important to ensure that weather will be favorable for data collection.
A handheld instrument for measuring temperature, relative humidity, and wind speed may also aid in the reporting of flight conditions, though note that the wind speed at flight altitude may be different than what is measured on the ground.
In many cases, taking a picture of the sky and a screenshot of the weather forecast from a reputable source (e.g., the National Oceanic and Atmospheric Administration) is a convenient and sufficient way to ensure later reporting on flight conditions.
In fact, the NEON AOP does exactly this for their daily flight reports.

## Flight planning

One of the key benefits of UAS operations is the ability to program missions to be automatically followed by the vehicle’s onboard flight software.
For optical data collection such as that required for SfM photogrammetry, the mission typically involves aerial transects with images captured at regular time or distance intervals so that objects in a scene are imaged from many viewing angles (often in excess of 100; `r fig_nums(name = "fig-crosshatch", display = "cite")`).
Successful flight planning requires consideration of the flight parameters, flight planning software, and operation routine.

The flight parameters are crucial determinants of whether or not the SfM photogrammetry will successfully create a digital model of the survey area.
Flight parameters are typically described in terms of the front and side overlap of the resulting imagery, as well as the sensor angle.
The front overlap is a function of flight speed, flight altitude, frequency of image capture, and the vertical field of view of the sensor, while the side overlap is a function of flight altitude, horizontal field of view of the sensor, and distance between transects.
Overlap in excess of 80% for both front and side overlap [@dandois2015] and even as high as 95% front overlap [@torres-sanchez2018; @frey2018] is required for successful photogrammetric reconstructions of more complex vegetation (such as denser forests) using commonly available photo processing software.
Lower overlap may be sufficient for 2-D mapping quality, though the processed product may not penetrate deeply into canopy gaps [@dandois2015] and image artifacts such as “leaning” objects which were only imaged from an oblique angle are more prevalent.
Additional overlap can be achieved by augmenting parallel transects with a second set of parallel transects rotated 90 degrees to the first (a cross hatch pattern; `r fig_nums(name = "fig-crosshatch", display = "cite")`).
Additional viewing angles can be achieved by tilting the sensor off nadir in order to capture oblique imagery which can aid in scene reconstruction [@james2014; @cunliffe2016].
Published work exists that determines optimal flight parameters for creating digital representations specific survey areas [@dandois2013; @ni2018; @frey2018; @torres-sanchez2018; @nesbit2019; @diaz2020; @swayze2021; @young2021], but it still may require some trial and error (or a new study) to optimize parameters for a new study area or system.

Flight planning is typically achieved using specialized software, sometimes run on a separate device like a tablet computer.
Most flight software allows for setting the altitude as well as the desired forward and side overlap for a given aircraft and sensor.
Two other important software features that may routinely be relevant for ecology are terrain following and internet-free operations.
Terrain following enables the vehicle to ascend and descend to match topographic changes within the area of interest, such that approximately the same altitude above ground level is maintained throughout all aerial transects.
This serves two key functions: it ensures the safety of the vehicle and it maintains approximately the same ground sampling distance for imagery which aids in processing.
Some missions are most easily created once in the field in order to incorporate better information on the area of interest, takeoff/landing locations, and visibility throughout flight.
An ability for the software to function offline and to cache background map imagery can be critical for real-world UAS use.
Flight software is resource intensive and generally requires a computer or tablet with relatively high computing power.
We have experienced flight software freezing mid-flight due to compute resource overload when using tablets that weren't up to the task, which can create a hazardous situation.
It is likely worth investing in a device with faster processors and/or more RAM.
Finally, some flight software provide additional functionality if the tablet has geolocation services-- an ability to determine its location on Earth by connecting with satellite networks.
For instance, the flight software may display the tablet's location on the background map during flight or even update the "home point" location for the UAS during the mission as the pilot moves around.
The home point is the location to which the UAS returns and lands after a mission is completed, a battery is depleted, or the pilot triggers a manual "return to home" command.
An updating home point might allow the pilot to traverse the landscape to stay closer to the UAS, thereby better maintaining visual line of sight or allowing the UAS to collect more data per flight since the travel distance to the landing point is minimized (during which time data typically aren't collected).
Not all tablets have geolocation services; as of this writing, the Cellular+WiFi version of the Apple iPad Pro has geolocation services, but the WiFi-only version does not.

A final consideration for successful flight planning is to create a routine for consistently executing missions.
Consistent repetition of routine steps prior to, during, and after a flight ensures that all components of the UAS work as intended in concert with each other, and checklists facilitate this consistency [@degani1993].
We highly recommend developing and using some kind of checklist for UAS operations (Supplementary Information)-- there is good reason they are part of standard operations for a range of aviators from pilots of small private aircraft to NEON AOP to NASA astronauts! Some applications (such as Kittyhawk; https://kittyhawk.io/) allow for automatic logging of checklist run-throughs, which further reduces barriers to their use.
For our mission, we used Map Pilot for DJI by DronesMadeEasy, which enables full control of front/side overlap and camera angle, as well as allows for crosshatch flight patterns, terrain following, and caching of data for use in the field when an internet connection isn’t available.

```{r}
fig_nums(name = "fig-crosshatch", "Black points depict the UAS position for each photo captured during the flight.
Red points in the “X” formation at the center are the high-precision geolocations of the NEON vegetation plot monuments.
The background color represents the approximate number of photos captured over each point in the surveyed area based on idealized image footprints projected on the ground surrounding the geolocation of each photo point (i.e., the black points).
Each point needs to be imaged a large number of times (likely more than 100 for denser vegetation), which means that some areas with image capture at the edges of the flown area won’t have coverage suitable for structure from motion data processing. 
The flight area should therefore be larger than the area of interest to ensure sufficient data coverage.")
```

![`r fig_nums(name = "fig-crosshatch")`](../../figs/fig01_photo-points-overlap-and-gcps.png)

## Regulations (airspace and pilot)

In the United States, use of UAS is constrained by regulations on pilot credentials, airspace in which operations take place, and vehicle type.
These restrictions are an oft-cited hurdle to adoption of UAS for research use [e.g., @vincent2015].

There are currently three main legal frameworks governing UAS operations within the United States: regulations for commercial operations (described in Title 14 of the Code of Federal Regulations Part 107, colloquially referred to as "Part 107"), regulations for recreational operations (described in Title 49 of the United States Code Section 44809), and regulations for a specific organization under a Certificate of Authorization (COA) granted by the Federal Aviation Administration (FAA).
The rules within each of these categories are actively changing, and drone pilots are responsible for staying aware of these updates.


COAs for organizations are generally labor intensive to set up and maintain, as they require ongoing coordination with the FAA, but they can allow for operations not typically permitted under other regulatory frameworks.
Part 107 credentials can be readily obtained by individuals without the need for complex organizational overhead as would be required for a COA.
Obtaining Part 107 credentials-- a "remote pilot certificate" from the FAA-- requires successful completion of an initial knowledge exam and needs renewing every 2 years.
Unlike permissions granted under a COA, the Part 107 credentials stay with the pilot and are transferable if the pilot changes organizations (e.g., a graduate student can't operate a UAS for research under a university's COA after they graduate, but they would still retain their ability to operate under Part 107).
As of this writing, the rules for flying under Part 107 are broadly permissive and allow for myriad opportunities to use UAS to collect ecological data.
Two of the most relevant restrictions for ecologists operating under Part 107 are: 1) the UAS must be within visual line-of-sight of the pilot in command (or within visual line of sight of another crew member acting as a "visual observer" as long as that observer has direct communication with the pilot in command), 2) the UAS must fly no higher than 400 feet (122 m) above ground level (AGL).
Part 107 does constrain operations in other ways (e.g., UAS can't fly faster than 87 knots [161 km/h], UAS must be at least 500 feet [152 m] below clouds and 2000 feet [609 m] horizontally from clouds), but high quality optical data collection usually requires UAS operations to be well within these limits.

Additional authorizations are needed to fly in "controlled" airspace (i.e., class B/C/D/E airspace, typically near airports), to fly a UAS above 55 lbs, and to fly a UAS beyond line-of-sight.
Some of these authorizations are relatively easy to obtain (e.g., many requests to fly in controlled airspace below 400 feet (122 m) AGL can be automatically granted in near real time using the Low Altitude Authorization and Notification Capability (LAANC)), while others are nearly impossible (at the time of this writing) and are likely beyond the reach of an ecological data collection campaign (e.g., beyond visual line-of-sight flights).
It is important to always call the appropriate land manager before flying on public land to obtain appropriate site access if necessary, to check for temporary closures (e.g., bird nesting), and to be a good neighbor.
Because NEON does not own the land on which they operate, flying NEON sites will require contacting and obtaining permission from the site host; contact information is available on the NEON webpage for each site, and NEON staff may also help facilitate those connections.
Additional, non-NEON research is allowed at some but not all sites.
If permission is obtained, it is important not to disturb any existing research being conducted at those sites, to maintain a 20m buffer around any NEON distributed plot, and to completely avoid the area of the tower airshed (which is also delineated on the NEON webpage for each site; e.g., https://www.neonscience.org/data-samples/data/spatial-data-maps).
Clear communication with concerned parties of UAS flights for research, even if there is every legal right to fly at a particular location, is important for building community credibility and longevity for UAS as a tool for ecologists.
Finally, as with flight planning, it is best practice to develop a routine and a checklist (see Supplementary Information) for determining whether UAS flights are allowed in the intended survey area under the relevant regulatory framework.

```{r}
fig_nums(name = "fig-rededge-rsr-neon-aop", "a) Relative spectral response of the MicaSense RedEdge 3 camera in 5 distinct spectral bands based on the quantum efficiency of the image sensor per wavelength and the bandpass filter transmission per wavelength. 
The dashed vertical lines in a) demarcate the spectral extent of panel b).
b) Relative spectral responses for two channels of the MicaSense RedEdge 3 camera plotted with the relative spectral responses for twenty channels of the NEON AOP imaging spectrometer.
Several channels of the NEON AOP instrument comprise each of the MicaSense RedEdge 3 channels, so the reflectance data from the NEON AOP are resampled (weighted, in effect) such that they can be used as though the NEON instrument exhibited the same spectral sensitivity as the MicaSense RedEdge 3 instrument.")
```

![`r fig_nums(name = "fig-rededge-rsr-neon-aop")`](../../figs/fig02_relative-spectral-response-rededge3-and-neon-aop.png)

## Radiometric Calibration and Corrections

Optical data from UAS-mounted sensors must be radiometrically calibrated in order to convert otherwise arbitrary image pixel values into meaningful, standardized units like reflectance.
Applying image preprocessing steps (e.g., correcting for camera artifacts such as vignetting and dark noise) and subsequent radiometric calibration allows our UAS data to be comparable with high-quality scientific data products derived from the NEON AOP.
The Empirical Line Method (ELM) has proved to be a simple and accurate UAS radiometric calibration option [@wang2015].
ELM requires the placement of at least two materials such as calibrated reflectance panels with known reflectance in the scene, which are imaged while the sensor is in flight.
These images containing the calibrated reflectance panels are then used to translate image pixel values to reflectance for each spectral band for the whole survey area.
For some sensors, particularly low cost multispectral sensors designed for agriculture, a downwelling light sensor (DLS a.k.a. sunshine sensor) also records data about the illumination levels at the exact moment that each image is captured.
This information is often incorporated into the SfM processing to partially correct for varying light conditions throughout the flight.
Importantly, the DLS can help account for varying illumination from image-to-image, but it doesn't allow for conversion of the image pixel values into a standardized unit of reflectance the way that calibrated reflectance panels can.

NEON implements a complex algorithm to convert its imaging spectrometer data to units of reflectance [@karpowicz2015] that is founded on a similar principle as ELM.
A series of vicarious calibration flights are conducted with the NEON AOP before and after every field season [@leisso2014].
They fly over two large tarps with 48% (medium gray) and 3% (black) reflectance, collect ground-based reflectance measurements of these tarps with an ASD, and use these data to verify the radiometric calibration of the NEON AOP Imaging Spectrometer (https://www.neonscience.org/data-collection/imaging-spectrometer).
The reflectance of these tarps is meant to represent the upper and lower bounds of reflectance typically seen in nature.
NEON's algorithm also compensates for the scattering and absorption of light as it travels through the atmosphere (e.g., haze, water vapor) on its optical path to the AOP.

Using three panels with varying gray levels will allow for the most flexibility in calibration methodology for UAS image data.
Ideally, panels should be large enough to be imaged during flight and contain an area of 10 x 10 pixels [@wang2015].
Panels should be matte (as opposed to shiny or glossy) with a smooth, horizontal surface [@smith1999].
Panel colors should be shades of black (near 0% reflectance) and gray, ideally covering the range of reflectance for the subject of interest.
White (near 100% reflectance) panels are not recommended since they can saturate and cause other issues [@cao2019].
For plant surveys, we recommend a medium grey, dark grey, and black target since vegetation tends to be about 50% average reflectance or medium gray.
Calibrated reflectance panels often come with the sensor to be integrated on the vehicle, but they can also be purchased separately made at home.
Care must be taken with homemade panels because even though they may appear a particular shade to the human eye (visible spectrum), they may not be a similar reflectance across all wavelengths observed by a multi-spectral or hyperspectral sensor.
Many studies have identified promising materials for homemade panels: plywood covered with matte paint [@rosas2020], gray linoleum, and black fine-weave cotton fabric [@cao2019].

Researchers have vastly different constraints for their budget, environmental conditions in the field, and equipment availability, so “good enough” may be more realistically attainable than the “ideal” radiometric calibration practices described above.
If in-flight panel photos are not possible or if only a small panel is available (as is often the case with panels that come with a sensor), photos of the panel can be captured either before or after flight.
Many off-the-shelf multispectral sensors only come with one small calibration panel, but having one panel is better than none even though this may limit the data calibration possibilities in the future.
Further, popular commercial SfM software like Agisoft Metashape and Pix4D may only accommodate one panel, so correcting UAS imagery with a single panel may be the only practical option.
When only a single calibration panel is used, choosing a gray panel (rather than a white or black one) helps to avoid crushing or clipping in under/over exposed images.

Regardless of panel cost, color, or material, it is critical to clean, remeasure, recalibrate, and/or replace them over time to ensure the most accurate reflectance calibration possible.
This is especially important when field work involves exposing panels to harsh environmental conditions with dirt, dust, sand, sun, and any other types of physical damage or degradation.
Illustrating this point, @scholl2021 remeasured a calibrated reflectance panel after three years of field work using a handheld Analytical Spectral Devices (ASD, ASD Inc., a Malvern PANalytical Company, Longmont, Colorado) FieldSpec 4 spectrometer.
`r fig_nums(name = "fig-calibration-panel", display = "cite")` depicts the manufacturer-provided panel reflectance spectrum from the time of purchase in 2017 (Micasense) compared to the reflectance spectrum measured three years later with the handheld ASD.
The reflectance of the panel has decreased by as much as 10% due to the presence of dirt and dust, especially in the shorter wavelengths.
The manufacturer advises against cleaning this make and model of calibration panel as it would force debris further into the pores of the panel material, though newer panels from this manufacturer can be cleaned (see https://support.micasense.com/hc/en-us/articles/360005163934-Calibrated-Reflectance-Panel-Care-Instructions).
In general, it is key to ensure that the panel reflectance data being used for radiometric calibration accurately represent the panel's actual reflectance, either by using the manufacturer-provided reflectance data for new/clean panels or by using updated reflectance measurements on a panel that can't be restored to its initial conditions.

```{r}
fig_nums(name = "fig-calibration-panel", "a) Reflectance of a calibrated reflectance panel as a function of wavelength. 
The black solid line corresponds to the manufacturer-provided reflectance spectrum representing the panel’s reflectance at the time of purchase.
The blue dashed line corresponds to remeasurements of the panel's reflectance spectrum in 2020 with a handheld spectrometer, after three years of field use.
The MicaSense RedEdge 3 spectral band ranges (blue, green, red, red edge, near infrared) are depicted as vertical bars of color.
The panel reflectance decreased between 2017 and 2020, with this decrease being more pronounced towards the shorter wavelengths.
b) A photograph of the calibrated reflectance panel measured in a), taken in 2020 after three years of field use. 
The change in reflectance between 2017 and 2020 is likely the result of accumulated dust and sand from the field, as seen partially wiped away on the lower, right corner of the panel's plastic case.
The difference between the manufacturer-reported panel reflectance and the actual reflectance after heavy use demonstrates the necessity to clean, remeasure, or replace calibration panels when performing radiometric calibration.")
```

![`r fig_nums(name = "fig-calibration-panel")`](../../figs/fig03_micasense-rededge3-calibrated-reflectance-panel-deterioration.png)

## Georeferencing

It is important to consider how the geographic positions of objects within the UAS survey are used to answer the research question.
Those positions can range from being globally accurate with precise correspondence to a location on the Earth (e.g., the tree is located at these coordinates, plus or minus 5 centimeters) to being relatively accurate with the spatial relationships and real-world distances between objects in the scene preserved but perhaps all frame-shifted by some amount compared to reality (e.g., the first tree is 5 meters away from the second tree, but all the trees are shifted 10 meters compared to their true on-the-ground coordinates).
In fact, it is possible for the SfM photogrammetry process to reconstruct 3D models and orthomosaics of the area of interest using visual cues in individual images alone without any geolocation data at all, resulting in a relative accuracy between objects in the scene but no ability to make real world measurements (e.g., the distance between the two trees is 5% of the map).
In order to infer units from these relative distances (e.g., to get the distance in meters), some measure of scale in the imagery is required.
Geolocating the SfM photogrammetry products in real-world space requires external information about the geolocation of each input image, such as from the Global Navigation Satellite System (GNSS).
Note that GNSS is the generic term for the network of satellites that offer global coverage of geospatial position, of which the US-owned Global Positioning System (GPS) is a part.
Most popular off-the-shelf vehicles and/or optical payloads have a basic GNSS antenna and receiver with an accuracy of <10 m, and the optical data collected will be automatically geotagged in the image metadata.
The automatic integration of these metadata in the most popular SfM photogrammetry software means that the second scenario described above-- relative spatial accuracy, but with SfM products frame-shifted by some amount similar in magnitude to the GNSS receiver accuracy-- is achievable with no extra steps by the user.
If greater accuracy is required than what is provided by the built-in GNSS receiver however, then additional steps are required.

Ground control points (GCPs), Real-time Kinematic (RTK) corrections, and Post-processed Kinematic (PPK) corrections are three solutions to accurately georeference images collected by the UAS.
GCPs are markers laid out on the ground with known geolocations that are visible in the UAS data and are used to tie the UAS imagery to real-world coordinates during the SfM processing step.
The GCP approach can only be as precise as the tool used to measure the geolocations of the GCPs in the field.
To improve upon the geolocation accuracy already in place by using image metadata geotags from the basic GNSS receiver that is likely onboard the UAS, a high-precision GNSS must be used to mark the geolocations of the GPS.
A high-precision GNSS may be prohibitively expensive, but could potentially be borrowed or rented from geodetic services (e.g. nonprofit UNAVCO allows equipment to be borrowed for NSF-funded projects for free).
Ideally, GCPs will be placed near edges or randomly throughout the mission area, but the density of GCPs is typically more important, with [@santana2021] finding that 10 GCPs in their 2 ha area of interest were needed for sub-7 cm precision (but 4 GCPs produced 16 cm precision at all flight heights and GCP spatial distributions).
@zimmerman2020 found that it was optimal to place GCPs in the corners of the study site, as well as at low and high elevations within the study site.
GCPs must be visible from the sensor, so it’s best to place them in bright and open areas.
Finding suitable locations in heavily forested areas with closed canopies can be challenging, therefore, it may be beneficial to expand survey areas to include suitable areas for GCPs if none can be found within the area of scientific interest.
Examples of effective GCPs are fabric swaths placed in an X, bright-colored bucket lids, or checkered mats (`r fig_nums(name = "fig-gcps", display = "cite")`).
GCPs with more conspicuous, precise points make for more precise geolocating because that specific point can be more easily matched between the field- and UAS-measured data.
For instance, trying to identify the exact center of a bright-colored bucket lid from aerial imagery might allow for 10 cm of mismatch with the exact point measured on the ground, the intersection of two, 5-cm wide pieces of cloth might allow for 5 cm of mismatch, and the crisp intersection of the white and black triangles might only allow for 1 cm of mismatch (`r fig_nums(name = "fig-gcps", display = "cite")`).
Because the field measurements of GCP locations can be a slow step, it might be advantageous to install permanent monuments at desirable GCP locations, measure their precise locations once, then reuse those same points during future data collection (e.g., if not the conspicuous marker itself, perhaps a more discrete piece of rebar that can have the actual GCP draped over top of it just prior to new data collection).
Pre-existing permanent (or semi-permanent) points may also be used if they can be readily measured on the ground and are visible from the air.
For example, NEON TOS plots have permanent markers that have been georeferenced with high precision (approximately 0.3 m) that can be used as GCPs if they are visible to the UAS (`r fig_nums(name = "fig-crosshatch", display = "cite")`).

RTK and PPK corrections augment the accuracy of a UAS' built-in GNSS receiver by correcting the noise inherent in the instrument using additional equipment and processing steps without the need for laying out GCPs and determining their locations.
This can result in massive time savings, particularly when surveying large areas.
For instance, @gillan2021 was able to survey and process data covering over 190 hectares of rangeland in approximately 30 days versus an estimated 141 days using a conventional UAS workflow, with an estimated 47 days saved just from using an RTK system versus GCPs.
Even with RTK and PPK corrections, it is still considered good practice to lay out some GCPs at precisely known locations, then quantify geolocation error in the final SfM products by measuring the difference between the field- and UAS-measured GCP locations.

```{r}
fig_nums(name = "fig-gcps", "Aerial RGB photograph captured using a DJI Phantom 4 Pro on January 23, 2020 at 120m of altitude above ground level.
The photo depicts three ground control points (GCPs) each of two different types in the center of the image: 1 m long spray painted orange cotton drop cloth in an “X” pattern and 1 x 1 m squares of cotton drop cloth spray painted with black triangles.
The GCPs are progressively more conspicuous under the canopy, in the shrub field, and on the dirt road.
The size of the area covered by the main photograph is approximately 180 m wide x 120 m high.")
```

![`r fig_nums(name = "fig-gcps")`](../../figs/example-gcps-with-inset.png)

## Data management

Image data collected from a UAS can quickly become "big data", and being intentional about data management will ease friction points at every step in the science workflow, from data collection to manuscript writing (`r fig_nums(name = "fig-data-management-schematic", display = "cite")`).
Having a ballpark idea of the total anticipated data storage requirements will help guide data storage hardware purchases such as Secure Digital memory cards (SD cards), external hard drives, internal hard drives, network attached storage (NAS), third-party cloud storage allotments, or university/organization-provided cloud storage allotments. 
Given the desired flight plan, number of survey areas, and payload (as determined by what meets the science requirements), it should be possible to estimate the amount of data that will be collected per flight, per survey area, and in total for the whole project.
It is best practice to adhere as closely as possible to the "3-2-1 Backup Rule", where three copies of the data exist with a local, accessible copy on two different devices (e.g., local computer and an external hard drive) and one copy off-site (e.g., a cloud backup service) [@ruggiero2012]. 

UAS optical data are typically collected on SD cards inserted in the sensor, so it is important to have enough empty SD cards prior to flights to accommodate the data being collected in the field.
Swapping out the SD card after each flight for an empty one is advisable, so that the only copy of freshly-collected imagery data isn't lost in the event of a UAS mishap on the next flight.
Frequently transferring data from the SD cards to both a laptop hard drive and an external hard drive in the field satisfies the backup rule on having data stored on "two different devices."
Storing those two devices in different locations while in the field (e.g., in two different vehicles, or in the trunk and under the car seat) might prevent some types of data loss (e.g., theft of one of the devices).
Once those data are transferred to other devices, it is safe to delete the images on the SD cards in order to reuse them.
It’s recommended to perform quality assurance (QA) checks on the images while it is still possible to re-collect data.
This could mean viewing the images on a laptop on-site, or while still on location near the field study site.
Check the data for obvious artifacts such as over or under exposure in images, that the number of images expected were collected, that file sizes appear consistent and reasonable, and that necessary metadata was captured with each image (e.g., the geolocation).
Generally a full QA assessment cannot be performed in the field due to time and computation limitations, but the field QA should be sufficient to ensure the images can be processed into desired products.
Some NEON sites (e.g., NIWO) have a field house that may be accessed, with permission, for laptop friendly workspaces and/or charging options.

Once the data collection is completed, data management can be broken into a quick access phase when data need to be readily available (`r fig_nums(name = "fig-data-management-schematic", display = "cite")`, Short-term storage), and a slower access phase which concerns the longer-term storage of both data and metadata (`r fig_nums(name = "fig-data-management-schematic", display = "cite")`, Long-term storage).
During quick access phase, the data should be as "close" to the workstation doing the SfM processing as possible-- ideally on a fast internal hard drive (e.g., a solid state drive) on the same computer as the SfM software.
Having a good long-term storage solution for the imagery (and derived data products) are important for the slower access phase, and having a copy of those data off-site will satisfy the 3-2-1 Backup Rule.
Some universities/organizations might already have storage infrastructure capable of accommodating vast data volumes and off-site backups (e.g., Research Computing storage).
If university/organization storage infrastructure isn't an option, third-party cloud storage (e.g., Cyverse) or data storage-specific compute hardware (e.g., network attached storage) are also good options.
For such high volumes of data, establishing "data levels" that characterize how derived each new processed product is makes them easier to navigate and work with [@wyngaard2019].
Typically, Level 0 represents raw data (the original images from the sensor in the case of optical data) and higher levels are derived from lower levels [e.g., Figure 4 in @koontz2021 shows data levels for optical data collected for a forest ecology project].

For public-facing storage, we suggest publishing all data product levels to a long-term data repository with a digital object identifier (DOI) in the open science spirit of broadening access to research (`r fig_nums(name = "fig-data-management-schematic", display = "cite")`, Public-facing).
Ideally, this includes the original raw images taken from UAS missions, which may be processed in the future to even higher quality products given the rapid advances in SfM photogrammetry software.
This can prove costly with particularly high data volumes, but it may be possible to rely on university/organization cyberinfrastructure resources, or other options that cater specifically to researchers aiming to practice open science principles (e.g., Cyverse, Open Science Framework). 

```{r}
fig_nums(name = "fig-data-management-schematic", "Planning a data management pipeline is a large upfront investment but can save time and money in the long-run, making it well-worth prioritizing up-front.
Considering the storage, backup, and sharing needs of the datasets you anticipate collecting and processing ensure data persistency and availability.
This data pipeline describes options and includes recommendations.
There are trade-offs at each decision point, so it is important to understand your data needs and budget.
For example, whereas building your own data management system may be more affordable and tailored to your needs, rented external storage systems will back up your data automatically and maintain the system and hardware requirements for you.")
```

![`r fig_nums(name = "fig-data-management-schematic")`](../../figs/fig05_data-management-and-processing.png)

## Data processing

One common approach for processing UAS-derived imagery such that it can be integrated with other data sources (e.g., NEON) is SfM photogrammetry, which converts the original images into data products such as a two-dimensional orthomosaic and a three-dimensional point cloud.
Many software applications are available for SfM photogrammetry that produce results of similar quality [@forsmoo2019], and many have steep discounts for research or educational use (e.g., Agisoft Metashape, Pix4DMapper).
Some free, open source options are also available (e.g., OpenDroneMap) and are steadily improving.
SfM photogrammetry can be CPU, RAM, disk drive, and GPU-intensive, so a workstation that balances these hardware components is ideal.
Higher-end gaming desktops are often sufficiently powerful workstations for processing images locally, but cloud-processing options also exist [e.g., university high performance compute resources, add-on capabilities of the specific SfM software purchased, Cyverse-- see @swetnam2018].
Even if most of the processing takes place in the cloud, it can still be beneficial to have a relatively powerful local machine in order to readily view and manipulate the resulting data products.

SfM workflows require myriad decisions about processing parameters, all of which might affect the quality of the resulting data products.
An excellent SfM guide has been published by the US Geological Survey (USGS) for the Agisoft Metashape software [@over2021], and some researchers have experimented with various SfM processing parameter combinations to empirically determine optimal parameter sets for particular use cases [@tinkham2021; @young2021] though some trial and error may still be required for new study systems.
Some software allow for automating the SfM processing using coding scripts, which then serve as the transparent and reproducible record of the workflow.
Other software workflows are based on a point-and-click graphical user interface (GUI), which requires the user to take note of the processing steps.
It will eliminate some friction points with resulting SfM products (particularly the three-dimensional point cloud) to work in a coordinate reference system that measures local distances in true units of distance (e.g., distance measured in meters with the Universal Transverse Mercator (UTM) coordinate reference system).
In any case, it is important to be consistent with the coordinate reference system for each of your data products (e.g., GNSS positions of GCPs, GNSS locations of UAS camera).
When working with optical data, it may be necessary to "spectrally resample" the high spectral resolution NEON AOP in order to match the sensor payload of the UAS, whose spectral resolution is likely coarser and not aligned with that of the NEON instrument (`r fig_nums(name = "fig-rededge-rsr-neon-aop", display = "cite")`).
Finally, calculating derived spectral indices such as the Normalized Difference Vegetation Index (NDVI; @rouse1973) from the original reflectance channels can help with data harmonization across multiple sensors by reducing some of their individual reflectance inaccuracies [@cao2019].

After the SfM workflow is completed, there are many options for further processing the resulting data products (e.g., orthomosaics, point clouds) such that they can be integrated with NEON.
Many free, open source software tools exist for working with geospatial data products produced by UAS and NEON including QGIS (https://qgis.org/en/site/) for visualization and GUI-based manipulation of raster and vector data types, CloudCompare (https://www.danielgm.net/cc/) for visualization and GUI-based manipulation of point clouds, and a suite of packages (https://cran.r-project.org/web/views/Spatial.html) for the `R` programming language [@rcoreteam2021].
Several packages have also been developed specifically for working with NEON data, including `neonUtilities` [@lunch2021], `neonhs` [@joseph2021a], `geoNEON` [@nationalecologicalobservatorynetwork2020], and `NeonTreeEvaluation` [@weinstein2021a].
A recent review by @atkins2022 describes the ecosystem of `R` packages available for working with forestry data, many of which are relevant for the types of geospatial data produced by UAS and NEON.
More generally, working with these kinds of high-resolution geospatial data, which are often classically "big", can benefit from following the few simple rules recently outlined by @joseph2021.

# Case study

## Science Requirements
Forest inventories describe the geolocation and physical attributes of individual trees, and provide critical information for management decision-making and advancing ecological theory [@young2021].
Remote sensing approaches to creating forest inventories can cover more area than field-based methods at a lower cost per area, and recent approaches still allow for characterization of individual trees [@weinstein2019].
The NEON TOS collects field-based forest inventory data (the "Woody Plant Vegetation Structure" data product; DP1.10098.001) and remote sensing data in their AOP that have been used to generate forest inventory data [@weinstein2020].
The field-based data are restricted to 20 x 20m field plots, while the AOP data cover dozens of square kilometers but at moderately coarse resolution (10 cm for RGB imagery, 1 m for imaging spectrometer data, 1 m for LiDAR data).
UAS have the capacity to fill in missing scales of observation for creating forest inventories by capturing a broader spatial extent than field-based NEON data, but at a finer spatial resolution than NEON AOP data.
With this as a motivation, here we present a case study where we collect and process UAS data coincident with a NEON TOS plot to create a forest inventory.
We then benchmark that forest inventory against the NEON TOS field data, and describe how to extract individual tree-scale spectral information that is comparable to that collected by the NEON AOP.
We use the previous section's "core principles" as a framework for describing our workflow, and provide all data and code to further aid our mental model building. 

## Vehicle

Our vehicle was a DJI Matrice 100 rotocopter with four propellers and a proven track record of safe, predictable flights.
The vertical takeoffs and landings of the rotocopter style drone allowed us to operate the vehicle from a clearing as small as the width of the dirt access road to the site.
We used a piece of plywood laid on the ground as a flat, stable takeoff platform that would also help to minimize the amount of dust kicked up by the rotor wash during takeoff and landing. 
The Matrice 100 has a relatively high lift capacity that allows for a payload to be integrated, and is heavier than many consumer rotocopters which makes it both more stable in windy conditions and more challenging to transport beyond a road.

## Payload

We captured imagery using two co-mounted sensors: a gimbal-stabilized DJI Zenmuse X3 RGB camera, and a MicaSense RedEdge 3 sensor, which is sensitive to electromagnetic radiation in five distinct spectral channels across the visible and near infrared wavelengths.
The DJI Zenmuse X3 camera has a focal length of 3.6 mm, a sensor width of 6.17 mm, and a sensor height of 4.55 mm.
The MicaSense RedEdge 3 sensor has a focal length of 5.5 mm, a sensor width of 4.8 mm, and a sensor height of 3.6 mm.
We used a fixed mount and a prebuilt integration kit for the MicaSense RedEdge 3 made by the sensor manufacturer to integrate with our vehicle.
This particular mount is angled such that the sensor faces exactly downward when the aircraft is tilted forward in flight, and the integration kit allows the sensor to share power with the vehicle batteries.
The RedEdge 3 sensor image capture mechanism is independent from the flight planning app or the vehicle's flight computer, though deeper integration with specific vehicles is possible with newer versions of the sensor.
Prior to flight, we connected to the RedEdge 3 sensor with a laptop via its built-in WiFi to verify that the sensor's onboard GNSS receiver was functioning properly and to initiate image capture.
We set the RedEdge 3 sensor to capture images at a rate of 1 image per second.
We set the DJI Zenmuse X3 camera to capture images at a rate of 0.5 images per second.
Using the quantum efficiency and filter bandpass sensitivity of an average RedEdge 3 sensor provided by MicaSense, we estimated the relative spectral response of the instrument which characterizes how the sensor captures light across the electromagnetic spectrum (`r fig_nums(name = "fig-rededge-rsr-neon-aop", display = "cite")`).
We provide the relative spectral response data in a format that makes it interoperable with the hsdar package [@lehnert2019].

## Environment
Our data collection took place on a single day under mostly sunny, light wind conditions on October 9th, 2019 starting at 2pm mountain daylight time.
We ideally would have flown closer to solar noon to minimize shadows in the imagery, particularly this late in the year.

## Flight Planning
We used Map Pilot for DJI, an Apple iPad iOS app, for planning the flight.
Map Pilot is a reliable, full-featured flight planning application that allows us to set flight parameters such as forward overlap, side overlap, and sensor angle.
We flew at an altitude of 100 m, and set the forward overlap to 95% and side overlap to 80% (based on the built-in DJI RGB camera, the Zenmuse X3).
We used a zero-degree sensor angle (i.e., downward/nadir facing), and added a perpendicular set of aerial transects to create a crosshatch flight pattern (`r fig_nums(name = "fig-crosshatch", display = "cite")`).
We opted to plan flights with particularly high overlap so that we had the option to remove photos at different intervals prior to SfM processing in order to test how various photo densities affected our UAS-derived forest inventory benchmark against NEON TOS field data.
The Map Pilot software determines flight parameters such as flight speed and distance between aerial transects based on the user-desired front/side overlap, which are themselves based on the built-in DJI camera.
We calculated these flight parameters as follows:

\begin{align*}
x_{ground, x3} &= a_{vehicle} \frac{x_{sensor, x3}}{f_{sensor, x3}} \\
y_{ground, x3} &= a_{vehicle} \frac{y_{sensor, x3}}{f_{sensor, x3}} \\ 
t_{vehicle} &= (1 - o_{side, x3})(x_{ground, x3}) \\
s_{vehicle} &= (1 - o_{front, x3})(y_{ground, x3})(s_{shutter, x3}) \\
\end{align*}

Where $x_{ground, x3}$ is the horizontal dimension of the Zenmuse X3 sensor's ground footprint in m, $y_{ground, x3}$ is the vertical dimension of the Zenmuse X3 sensor's ground footprint in m, $a_{vehicle}$ is the vehicle's altitude during image capture in m, $x_{sensor, x3}$ is the width of the Zenmuse X3 sensor in mm, $y_{sensor, x3}$ is the height of the Zenmuse X3 sensor in mm, $f_{sensor, x3}$ is the focal length of the Zenmuse X3 sensor, $t_{vehicle}$ is the transect spacing between aerial transects of the vehicle in m, $o_{side, x3}$ is the planned side overlap of the Zenmuse X3 imagery as a fraction, $o_{front, x3}$ is the planned front overlap of the Zenmuse X3 imagery as a fraction, $s_{vehicle}$ is the speed of the vehicle in m/s, and $s_{shutter, x3}$ is the planned shutter speed of the Zenmuse X3 sensor.

Because the MicaSense RedEdge 3 has a different optical geometry than the Zenmuse X3 camera, we can use the flight parameters calculated above to determine the actual overlap of the imagery from the MicaSense RedEdge 3:

\begin{align*}
x_{ground, RE3} &= a_{vehicle} \frac{x_{sensor, RE3}}{f_{sensor, RE3}} \\
y_{ground, RE3} &= a_{vehicle} \frac{y_{sensor, RE3}}{f_{sensor, RE3}} \\
o_{side, RE3} &= 1 - \frac{t_{vehicle}}{x_{ground, RE3}} \\
o_{front, RE3} &= 1 - \frac{s_{vehicle}}{(s_{shutter, RE3})(y_{ground, RE3})} \\
\end{align*}

Where $x_{ground, RE3}$ is the horizontal dimension of the MicaSense RedEdge 3 sensor's ground footprint in mm, $y_{ground, RE3}$ is the vertical dimension of the RedEdge 3 sensor's ground footprint in m, $x_{sensor,RE3}$ is the width of the RedEdge 3 sensor in mm, $y_{sensor, RE3}$ is the height of the RedEdge 3 sensor in mm, $f_{sensor, RE3}$ is the focal length of the RedEdge 3 sensor, $o_{side, RE3}$ is the calculated side overlap of the RedEdge 3 imagery as a fraction, $o_{front, RE3}$ is the calculated front overlap of the RedEdge 3 imagery as a fraction, and $s_{shutter, RE3}$ is the planned shutter speed of the RedEdge 3 sensor.

Using the front and side overlap, we can estimate the approximate number of images captured of each point within the survey area for the Zenmuse X3 sensor, $p_{x3}$, and the MicaSense RedEdge 3 sensor, $p_{RE3}$, as:

\begin{align*}
p_{x3} &= \frac{1}{(1 - o_{side, x3})(1 - o_{front, x3})} \\
p_{RE3} &= \frac{1}{(1 - o_{side, RE3})(1 - o_{front, RE3})}
\end{align*}

The code for these calculations can be found at https://github.com/mikoontz/neon-drone-workflow/blob/master/workflow/02_preprocess-drone-data/03_drone_L0_image-overlap-calculator.R.

Our aerial transects were 17.14 m apart, our vehicle flew at 3.16 m/s, the side overlap of the RedEdge 3 imagery was 80.4%, and the front overlap of the RedEdge 3 imagery was 95.2%.
The estimated number of photos per point in the survey area was 200 for the Zenmuse X3 camera, and 105.5 for the MicaSense RedEdge 3 sensor.
The crosshatch flight plan effectively doubles the expected number of photos per point to 400 for the X3 camera and 211.0 for the RedEdge 3 (`r fig_nums(name = "fig-crosshatch", display = "cite")`).

## Regulations

We obtained permission to access the NIWO NEON site from the site host, the University of Colorado Boulder Mountain Research Station, as well as NEON itself.
We flew under the FAA Part 107 rules for commercial drone operations with a current remote pilot certificate, and ensured that the airspace was free for operating the UAS.

## Radiometric Calibration

The MicaSense RedEdge 3 multispectral camera comes with a small, grey calibrated reflectance panel that reflects approximately 60% of light across the entire spectral extent captured by the sensor.
We held the UAS over the panel and captured an image of the calibration panel prior to flight ensuring our shadow didn't cover the panel.
The RedEdge 3 also integrates a DLS, which faces upwards and measures illumination at the same time as the downward-facing image capture.
We included the calibration panel photos in the SfM processing workflow and also enabled the image-to-image corrections from the DLS.
When loading the calibration panel photos into the SfM software, we set the "known reflectance" of the panel in each of the five spectral channels to be those that we measured for this particular panel (`r fig_nums(name = "fig-calibration-panel", display = "cite")`), rather than those provided by the manufacturer.

## Georeferencing

We laid out orange cloth X's over the 9 permanent markers within the NIWO_017 field site (red points in `r fig_nums(name = "fig-crosshatch", display = "cite")`).
Five of these points were visible from the air.
These ground control points were located within the center of the flight area, without any geolocation representation at the edges which wasn't ideal [@zimmerman2020; @santana2021].

## Data Collection

For data collection, we recorded each flight’s imagery on a separate 32 GB SD card rated at >90 MB/s write speed.
For multi-day trips or if SD cards need to be reused, we transfer imagery from the SD cards to at least one portable solid state hard drive (Samsung T series).
Upon returning from the field, we transferred images from the SD cards (or portable solid state hard drive, as the case may be) to two locations: 1) the solid state hard drive on a local desktop gaming computer for short term storage and processing, and 2) a Network Attached Storage (NAS) with 6 spinning disk hard drives in a RAID array for long-term storage.
Both the short-term storage (local desktop) and long-term storage (NAS) solutions are backed up to the cloud using a 3rd party backup client (Backblaze) at a cost of ~$5.00 USD per terabyte per month.
We use the same data levels as @koontz2021, except we didn't process our data to Level 4.
To allow for future data collection to integrate easily into this project, we compartmentalized each data product to a folder for the specific flight date (2019-10-09), which was housed in a folder for the specific flight location (NIWO_017).
We used the Open Science Framework for public-facing storage (available upon publication).

## Data Processing

We used a local desktop computer (Alienware Aurora R7 with an Intel i7-8700k 3.70 GHz hexacore processor and 64 GB of RAM) for data processing.
We followed the USGS workflow to process our raw MicaSense RedEdge 3 imagery into a digital surface model, an orthomosaic and a dense point cloud using Agisoft Metashape version 1.6.1 [@over2021].
We noted each step in the SfM process, as well as the parameter choices we made, in a .txt file (https://github.com/mikoontz/neon-drone-workflow/blob/master/workflow/03_structure-from-motion-of-drone-data/01_drone_agisoft-metashape-processing-steps.txt).
We created a script to allow readers to download cropped versions of these SfM products that are relatively small in size in order to follow along with our post-SfM processing steps (https://github.com/mikoontz/neon-drone-workflow/blob/master/workflow/04_get-processed-example-drone-data/01_get-example-cropped-L1-and-L2-data.R).
We used `R` for all post-SfM steps, particularly the `sf` package for working with vector data [@pebesma2018] and the `terra` package [@hijmans2021] for working with raster data.
The `terra` package is intended to be a replacement for the `raster` package [@hijmans2021a], but some other `R` packages haven't yet migrated their codebase to use `terra`.
In these cases, we coerce `terra` objects to be `raster` objects in order to preserve the interoperability of the various packages.

We classified the dense point cloud into "ground" and "non-ground" points using a cloth simulation filter algorithm [@zhang2016] implemented in the `lidR` [@roussel2021] package.
Using the ground points, we interpolated a digital terrain model (DTM) representing the height of the ground (without the vegetation).
We subtracted this DTM from the SfM-derived digital surface model to create a canopy height model (CHM) representing the height of the vegetation in the survey area.

To integrate our UAS data with NEON TOS field data, we used the CHM to detect and segment individual tree crowns.
We used a variable window filter to detect individual trees, which searches for all of the maximum heights in a CHM within a circle of variable radius defined by the height of each pixel in that CHM in turn [@popescu2004].
That is, pixels in the CHM representing taller vegetation will have a broader search radius within which the location of the maximum height is determined. 
We used the `ForestTools` package to implement this algorithm and used the optimal tree detection parameter set determined by @young2021 for a structurally complex mixed-conifer forest, with a variable search window defined using the following function: 

\begin{align*}
r &= 0.04x
\end{align*}

Where $r$ is the radius of the variable search window, and $x$ is the canopy height of the focal pixel.

Using the detected trees, we implemented a marker controlled watershed segmentation algorithm using the `ForestTools` package to segment individual tree crowns [@plowright2021].
For each tree crown, we created a new geometry representing its bounding box (i.e., the smallest rectangle that fully contains the irregularly-shaped crown polygon) in order to compare UAS-derived crown segments to those derived using deep learning approaches [@weinstein2019].
We benchmarked our tree detection using the `NeonTreeEvaluation` package [@weinstein2021a] to compare our tree detections with the NEON TOS field-collected tree locations and with previously-annotated crown bounding boxes derived from NEON AOP imagery [@weinstein2021a].
The stem locations of the trees in the NEON TOS sites can be determined using a combination of the `neonUtilities` package [@lunch2021] to download the and the `geoNEON` package [@nationalecologicalobservatorynetwork2020], but they are also directly available in the `NeonTreeEvaluation` package.
Within the NIWO_017 plot, we detected 60% of the field-measured stems (i.e., a recall score of 0.6).
Comparison with the annotated crowns using the `compute_precision_recall()` function in the `NeonTreeEvaluation` package also provides precision scores, which measures the amount of false positive tree identifications and which can be combined with the recall score in an integrated measure of predictive ability called the F-score:

\begin{align*}
F &= \frac{2 * p * r}{p + r}
\end{align*}

Where $F$ is the F-score, $p$ is the precision, and $r$ is the recall.

For our comparison, we set the threshold argument of the `compute_precision_recall()` function to 0.1 such that a predicted tree was considered correctly predicted if the intersection of its bounding box with an annotated crown's bounding box divided by the area of the union of those bounding boxes is greater than 0.1.
Our UAS-derived map of detected trees had a recall of 0.788 and a precision of 0.276, resulting in an F-score of 0.409.
For comparison, the DeepForest algorithm's predictions for the locations of trees at NIWO_017 [@weinstein2021a] had a recall of 0.861, a precision of 0.798, and an F-score of 0.828.
The poorer performance of the UAS-derived tree detection approach suggests that a different combination of flight parameters, SfM photogrammetry parameters, or tree detection algorithm/parameters might be better suited to the subalpine forest at NIWO [@young2021].

To integrate our UAS data with NEON AOP reflectance data, we calculated NDVI from each sensor.
We used the `neonUtilities` package to download the NEON AOP imaging spectrometer data (data product DP3.30006.001) that covers the NIWO_017 site from 2019, using the easting and northing of the centroid of the NIWO_017 plot and a 20 m buffer as argumeents to the `byTileAOP()` function [@lunch2021].
We used the `neonhs` [@joseph2021a] package to convert the raw NEON AOP data product into a raster object more readily manipulatable in `R`. 
Because the imaging spectrometer spectral response overlaps with, but doesn't perfectly align with the spectral response of the MicaSense RedEdge 3 sensor, we spectrally resampled the NEON AOP data to match the spectral resolution of the MicaSense RedEdge 3 sensor using the `hsdar` package [@lehnert2019] and the relative spectral response that we derived (`r fig_nums(name = "fig-rededge-rsr-neon-aop", display = "cite")`).
We used the UAS-derived orthomosaic and the spectrally-resampled NEON AOP orthomosaic to calculate NDVI.
`r fig_nums(name = "fig-comparing-scenes", display = "cite")` shows the comparison between NDVI as captured by the NEON AOP flight in August with our UAS-derived NDVI from our flight in October.
We used the `exactextractr` package to extract the mean and standard deviation of NDVI derived from the UAS as well as the spectrally resampled NEON AOP for each segmented tree crown [@danielbaston2021].
`r fig_nums(name = "fig-crown-ndvi-compare", display = "cite")` shows the comparison of NDVI derived from the NEON AOP and the UAS at an individual tree scale. 


```{r}
fig_nums(name = "fig-comparing-scenes", "NDVI derived from the NEON AOP spectral imager using data collected in August of 2019 (data collection flights over NIWO on August 14, 15, 19, and 26 of 2019) and the MicaSense RedEdge 3 camera using data collected on October 09, 2019.
The NEON AOP data were first spectrally resampled into the equivalent red and near infrared bands of the MicaSense RedEdge 3 camera based on the relative spectral response of the RedEdge 3 instrument.
The higher spatial resolution of the drone-derived data are highlighted.
Note that the difference in NDVI between the images may derive from three main sources: phenological differences in the vegetation, differences in the flight conditions such as time of day and cloud cover, or differences in instrumentation.")
```

![`r fig_nums(name = "fig-comparing-scenes")`](../../figs/ndvi_neon-spectral-resampled-v-drone-original.png)

```{r}
fig_nums(name = "fig-crown-ndvi-compare", "NDVI for each detected tree crown in the NIWO_017 plot as derived from the NEON AOP imaging spectrometer versus NDVI derived from the MicaSense RedEdge 3 sensor.
The one-to-one line is red, and the best fit curve from a general additive model is shown in blue with a 95% confidence interval shown in grey.
The NEON AOP data, collected closer to peak growing season, mostly has higer per-tree values of NDVI.
The nonlinear model fit highlights how, at the individual tree scale, spectral responses vary differently throughout the year which might prove useful for future work (e.g., classifying trees to species based on how variable their spectral response is through the year).")
```

![`r fig_nums(name = "fig-crown-ndvi-compare")`](../../figs/uas-vs-neon-ndvi-per-crown.png)


# Discussion

Macroecology will benefit from a “macroscope” to enable the study of broad-extent phenomena across multiple scales of biological, geophysical, and social processes [@beck2012; @lawton1996; @dornelas2019]. 
The ideal macroscope comprises a nested array of tools that provide full coverage of spatial and temporal observational domains. 
In their complementarity, the value of multiple observational tools in concert is more than the sum of the parts [@dornelas2019]. 
Pairing UAS with NEON partially completes the constellation of Earth observing tools that contribute to the macroscope, and combines the flexibility of UAS with the high-quality and consistency of NEON. 
In this work, we aid the adoption of these tools among macroecologists by providing a mental model-- a contextual framework-- and some practical considerations for their integration.

Challenges remain for integrating UAS with NEON, but they are surmountable.
Some of these challenges are fundamentally associated with "big", cross-scale data.
Integrating data across scales brings a host of potential pitfalls that could pollute inference if care isn't taken to avoid them [@zipkin2021].
Clear documentation of data provenance including sensor characteristics, data acquisition methods (e.g., flight pattern), and data acquisition conditions (e.g., time of day, cloud cover) will enable rigorous cross-scale data integration.
The proliferation of reasonably low-cost, off-the-shelf, drone-ready sensors (many designed for precision agriculture use) creates a need for validation of whether those instruments produce "science grade" data (which itself is a relative term, depending on what the specific science requirements are for a given project).
This validation may be achieved via direct comparison of the low-cost sensors with “state of the science” instruments using coincident flights [e.g., @fawcett2020].
Big data in ecology is relatively new [@farley2018], and approaches to UAS-derived big data are fairly ad hoc across researchers [@wyngaard2019].
Maintaining supportive communities of practice, such as the High Latitude Drone Ecology Network (https://arcticdrones.org/), can help overcome some of these idiosyncratic approaches.
Integrating UAS operations with NEON can help anchor the community to the common currency of NEON data types, organization, and collection protocols which will enhance the interoperability of UAS data.
Cyberinfrastructure for managing and processing UAS data is not yet built in a way that encourages consistency between projects or researchers.
In this way, NEON provides an aspirational example for how purpose-built cyberinfrastructure can facilitate macroecology, and the foundational resources for building an equivalently valuable architecture for UAS data may already be represented in other NSF-sponsored projects (e.g., Cyverse, OpenTopography, Open Science Framework). 
NEON also provides an aspirational target for educational resources, which are critical to ensuring that would-be NEON/UAS users have the environmental data science skills necessary to turn their data into inference [@hampton2017].
Critically, reducing some barriers to access to an elusive observational domain (broad-extent, fine-grain) doesn’t equate to it being fully accessible. 
We can illustrate this point using access to the Landsat archive, which we previously discussed as an exemplar of the effect of data access on democratization of science. 
While the reduced cost of Landsat images brought more researchers from lower prestige institutions and lower income parts of the world into the user base [@nagaraj2020], those users are still overwhelmingly men [@miller2016; @miller2016a].
A truly accessible tool wouldn't exhibit such a demographic bias, and it is incumbent upon the UAS and NEON community to be self-reflective about who is being excluded from the community and proactive about what steps can be taken to make it more inclusive.

We conclude with a research agenda for UAS/NEON integration paired with example ecology applications, which we hope provides a vision to be built upon:  

1. Filling in spatial scales missed by NEON data collection (e.g., collecting data on a similar vegetation type of a NEON site but outside of NEON's direct footprint, capturing data at spatial resolutions finer than 10 cm in order to measure post-disturbance vegetation recovery)
1. Filling in temporal scales missed by NEON data collection (e.g., capturing data in a year when a NEON site is skipped by the AOP, capturing data at a site multiple times per year to understand how snowpack changes through the year, tracking individual plant phenology through time and linking to phenocam data)
1. Opportunistic data collection (e.g., capturing data immediately after a disturbance event to measure its severity)
1. Connecting NEON data to other Earth observing systems using UAS data as a bridge (e.g., spectrally unmixing Landsat pixels to determine relative species compositions by matching UAS spectral measurements to NEON TOS field measurements)
1. Supplementing NEON data using sensors that aren't part of the NEON suite of sensors (e.g., thermal data to compare thermal regulation of different plant species, measuring water stress in different trees across gradients of topoclimate) 
1. Validating lower cost, off-the-shelf payloads against the state-of-the-science NEON data collection (e.g., determining how well a multispectral imager designed for agriculture captures surface reflectance, determining how well an algorithm detects the trees in a NEON vegetation structure plot)
1. Replacing high-cost NEON AOP flights with lower-cost alternatives (e.g., if the drone-derived data are "good enough" compared to the AOP, can we reduce the operational costs of the AOP)
1. Using NEON data as a common currency for validating new methods (e.g., the case study we showed here, comparing a deep learning/orthomosaic based approach and a variable window filter/canopy height model approach to detecting individual trees measured by the NEON TOS)

# Acknowledgements {-}

Funding for the 2019 NEON Science Summit was provided by NSF Award #1906144. Additional funding was provided by Earth Lab through the University of Colorado, Boulder’s Grand Challenge Initiative, the Cooperative Institute for Research in Environmental Sciences, and the North Central Climate Adaptation Science Center. We would also like to acknowledge the National Aeronautics and Space Administration (NASA) New Investigator Program (NIP) grant 80NSSC18K0750 to M. E. Cattau. The National Ecological Observatory Network is a program sponsored by the National Science Foundation and operated under cooperative agreement by Battelle. This material is based in part upon work supported by the National Science Foundation through the NEON Program. We thank Chelsea Nagy, Jeff Sloan, and two anonymous reviewers for their feedback, which greatly improved the manuscript.

<!-- ## Author contributions -->

<!-- Author contributions are defined using the Contributor Roles Taxonomy (CRediT; https://casrai.org/credit/). -->
<!-- Conceptualization: ; Data curation: ; Formal analysis: ; Funding acquisition: ; Investigation: ; Methodology: ; Project administration: ; Resources: ; Software: ; Supervision: ; Validation: ; Visualization: ; Writing -- original draft: ; Writing -- review and editing:    -->

\newpage

# References {-}